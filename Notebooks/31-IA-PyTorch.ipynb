{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1>Inteligencia Artificial</h1>\n",
    "    <h1>PyTorch</h1>\n",
    "    <br>\n",
    "    <h5>Prof. Wladimir Rodríguez</h5>\n",
    "    <h5>wladimir.rodriguez@outlook.com</h5>\n",
    "    <h5>Departamento de Computación</h5>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../figuras/PyTorch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ¿Qué es [PyTorch](www.pytorch.org)?\n",
    "\n",
    "Es un paquete de computación científica basado en Python dirigido a dos conjuntos de\n",
    "audiencias:\n",
    "- Un reemplazo para NumPy para usar el poder de las GPU\n",
    "- Una plataforma de investigación de aprendizaje profundo que proporciona la máxima flexibilidad\n",
    "    y velocidad\n",
    "    \n",
    "## Empezando\n",
    "\n",
    "### Tensores\n",
    "\n",
    "Los tensores son similares a los `ndarrays` de NumPy, con la adición de que los tensores también se pueden usar en una GPU para acelerar la computaciȯn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchviz import make_dot\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from time import time\n",
    "%matplotlib inline\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota\n",
    "\n",
    "> Se declara una matriz no inicializada, pero no contiene valores conocidos definidos antes de ser utilizada. Cuando se crea una matriz no inicializada, los valores que estaban en la memoria asignada en ese momento aparecerán como valores iniciales.\n",
    "\n",
    "Construya una matriz de 5x3, sin inicializar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.5889e-12, 4.5576e-41, 5.6512e-38],\n",
       "        [3.0666e-41, 0.0000e+00, 0.0000e+00],\n",
       "        [3.9096e-43, 3.0666e-41, 5.5586e-38],\n",
       "        [3.0666e-41, 5.5586e-38, 3.0666e-41],\n",
       "        [3.5032e-43, 0.0000e+00, 3.9376e-43]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.empty(5, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construya una matriz inicializada al azar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1603, 0.2320, 0.7056],\n",
       "        [0.1571, 0.4322, 0.5991],\n",
       "        [0.7896, 0.1840, 0.1943],\n",
       "        [0.8480, 0.8820, 0.5845],\n",
       "        [0.1397, 0.9240, 0.1812]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construya una matriz llena de ceros y de `dtype long`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(5, 3, dtype=torch.long)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construya un tensor directamente de los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.5000, 3.0000])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([5.5, 3])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "o crear un tensor basado en un tensor existente. Estos métodos reutilizarán las propiedades del tensor de entrada, a menos que el usuario proporcione nuevos valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.6306, 0.6906, 0.7795],\n",
       "        [0.7156, 0.9895, 0.5812],\n",
       "        [0.5263, 0.6495, 0.0716],\n",
       "        [0.7275, 0.1763, 0.3134],\n",
       "        [0.2281, 0.8006, 0.7141]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.new_ones(5, 3, dtype=torch.double)    # los métodos new_* toman tamaños\n",
    "print(x)\n",
    "x = torch.rand_like(x, dtype=torch.float)   # cambiar dtype!\n",
    "x                                           # el resultado tiene el mismo tamaño"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtener su tamaño:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota\n",
    "\n",
    "> `torch.Size` es de hecho una tupla, por lo que admite todas las operaciones de tupla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operaciones\n",
    "\n",
    "Existen múltiples sintaxis para las operaciones. En el siguiente ejemplo, veremos la operación de adición.\n",
    "\n",
    "Adición: sintaxis 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3952, 1.0348, 0.9814],\n",
       "        [1.1302, 1.9312, 1.4676],\n",
       "        [1.4077, 0.7895, 0.4333],\n",
       "        [1.1837, 0.5409, 0.7567],\n",
       "        [0.7298, 0.8062, 1.4674]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand(5, 3)\n",
    "x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adición: sintaxis 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3952, 1.0348, 0.9814],\n",
       "        [1.1302, 1.9312, 1.4676],\n",
       "        [1.4077, 0.7895, 0.4333],\n",
       "        [1.1837, 0.5409, 0.7567],\n",
       "        [0.7298, 0.8062, 1.4674]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adición: proporcionar un tensor de salida como argumento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3952, 1.0348, 0.9814],\n",
       "        [1.1302, 1.9312, 1.4676],\n",
       "        [1.4077, 0.7895, 0.4333],\n",
       "        [1.1837, 0.5409, 0.7567],\n",
       "        [0.7298, 0.8062, 1.4674]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultado = torch.empty(5, 3)\n",
    "torch.add(x, y, out=resultado)\n",
    "resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adición: en sitio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3952, 1.0348, 0.9814],\n",
       "        [1.1302, 1.9312, 1.4676],\n",
       "        [1.4077, 0.7895, 0.4333],\n",
       "        [1.1837, 0.5409, 0.7567],\n",
       "        [0.7298, 0.8062, 1.4674]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.add_(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota\n",
    "\n",
    "> Cualquier operación que mute un tensor en el lugar se le coloca el postfijo \\_. Por ejemplo: *x.copy_(y)*, *x.t_()*, cambiará *x*.\n",
    "\n",
    "¡Puede usar la indexación NumPy estándar con todas las campanas y silbatos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6906, 0.9895, 0.6495, 0.1763, 0.8006])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cambio de tamaño: si desea cambiar el tamaño / remodelar el tensor, puede usar `torch.view`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8)\n",
    "print(x.size(), y.size(), z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si tiene un tensor de un elemento, use `.item()` para obtener el valor como un número de Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.8026])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.802563488483429"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1)\n",
    "print(x)\n",
    "x.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leer mas tarde:\n",
    "\n",
    "> [Aquí](https://pytorch.org/docs/stable/torch.html) se describen más de 100 operaciones de tensor, que incluyen transposición, indexación, corte, operaciones matemáticas, álgebra lineal, números aleatorios, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Puente NumPy\n",
    "\n",
    "Convertir un Tensor Torch en una matriz NumPy y viceversa es muy fácil.\n",
    "\n",
    "El Tensor Torch y la matriz NumPy compartirán sus ubicaciones de memoria subyacentes (si el Tensor Torch está en la CPU), y cambiar una cambiará la otra.\n",
    "### Convertir un Tensor Torch en una matriz NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.numpy()\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vea cómo la matriz numpy cambió de valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversión de matriz NumPy a Tensor Torch\n",
    "\n",
    "Vea cómo cambiar la matriz NumPy cambió el Tensor Torch automáticamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todos los tensores en la CPU excepto un CharTensor admiten la conversión a NumPy y viceversa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensores CUDA\n",
    "\n",
    "Los tensores se pueden mover a cualquier dispositivo utilizando el método `.to`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejecutemos esta celda solo si CUDA está disponible\n",
    "# Usaremos objetos ``torch.device`` para mover los tensores dentro y fuera de la GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # un objeto dispositivo CUDA\n",
    "    y = torch.ones_like(x, device=device)  # crear directamente un tensor en la GPU\n",
    "    x = x.to(device)                       # o usar la cadena ``.to(\"cuda\")``\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un problema de regresión simple\n",
    "\n",
    "Vamos a comenzar a utilizar `PyTorch` con un problema sencillo de regresión lineal con una sola característica $x$. \n",
    "\n",
    "$$y = a +bx + \\epsilon$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generacion de datos\n",
    "\n",
    "Comencemos generando algunos datos sintéticos: comenzamos con un vector de 100 puntos para nuestra característica $x$ y creamos nuestras etiquetas usando $a = 1$, $b = 2$ y algo de ruido gaussiano.\n",
    "\n",
    "A continuación, dividamos nuestros datos sintéticos en conjuntos de entrenamiento y validación, barajando la matriz de índices y utilizando los primeros 80 puntos barajados para el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generación de Datos\n",
    "np.random.seed(42)\n",
    "x = np.random.rand(100, 1)\n",
    "y = 1 + 2 * x + .1 * np.random.randn(100, 1)\n",
    "\n",
    "# Barajar los indices\n",
    "indices = np.arange(100)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Usar los primeros 80 indices aleatorios para entrenamiento\n",
    "indices_entrenamiento = indices[:80]\n",
    "# Usar los indices restantes para validación\n",
    "indices_validacion = indices[80:]\n",
    "\n",
    "# Generar conjuntos de entrenamiento y validación\n",
    "x_entrenamiento, y_entrenamiento = x[indices_entrenamiento], y[indices_entrenamiento]\n",
    "x_validacion, y_validacion = x[indices_validacion], y[indices_validacion]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualización de los conjuntos de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "figura, axs = plt.subplots(1, 2)\n",
    "axs[0].scatter(x_entrenamiento, y_entrenamiento)\n",
    "axs[0].set(xlabel='x', ylabel='y')\n",
    "axs[0].set_title('Datos Entrenamiento')\n",
    "axs[1].scatter(x_validacion, y_validacion, c='red')\n",
    "axs[1].set(xlabel='x', ylabel='y')\n",
    "axs[1].set_title('Datos Validación')\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargando datos, dispositivos y CUDA\n",
    "\n",
    "Para convertir las matrices Numpy a tensores Pytorch usaremos `from_numpy`. Sin embargo, este método devuelve un tensor de CPU.\n",
    "\n",
    "Si queremos utilizar la GPU podemos usar el método `to()`. Envía un tensor a cualquier dispositivo que especifique, incluida la GPU (denominada `cuda` o `cuda:0`).\n",
    "\n",
    "Para hacer que el código use la CPU en caso de que no tengamos disponible una GPU se puede usar `cuda.is_available()` para averiguar si tiene una GPU a su disposición y configura tu dispositivo en consecuencia.\n",
    "\n",
    "También puede convertirlo fácilmente a una precisión menor (flotante de 32 bits) utilizando `float()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'torch.Tensor'> torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "dispositivo = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Nuestros datos estaban en matrices Numpy, pero tenemos que transformarlos en los tensores de PyTorch\n",
    "# y luego los enviamos al dispositivo elegido\n",
    "x_entrenamiento_tensor = torch.from_numpy(x_entrenamiento).float().to(dispositivo)\n",
    "y_entrenamiento_tensor = torch.from_numpy(y_entrenamiento).float().to(dispositivo)\n",
    "\n",
    "# Aquí podemos ver la diferencia: observe que .type() es más útil\n",
    "# ya que también nos dice DÓNDE está el tensor (device)\n",
    "print(type(x_entrenamiento), type(x_entrenamiento_tensor), x_entrenamiento_tensor.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si comparamos los tipos de ambas variables, obtendrá lo que esperaría: `numpy.ndarray` para la primera y `torch.Tensor` para la segunda.\n",
    "\n",
    "El tipo de dato de un tensor de GPU sería `torch.cuda.FloatTensor`.\n",
    "\n",
    "También podemos dar la vuelta, volviendo los tensores a matrices Numpy, usando `numpy()`. Debería ser fácil como `x_train_tensor.numpy()` pero ...\n",
    "\n",
    "> TypeError: no se puede convertir el tensor CUDA en numpy. Use `Tensor.cpu()` para copiar primero el tensor a la memoria del host.\n",
    "\n",
    "Desafortunadamente, Numpy no puede manejar los tensores de GPU ... primero debe hacer que sean tensores de CPU usando `cpu()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creando Parámetros\n",
    "\n",
    "¿Qué distingue un tensor utilizado para datos, como los que acabamos de crear, de un tensor utilizado como parámetro / peso (entrenable)?\n",
    "\n",
    "Los últimos tensores requieren el cálculo de sus gradientes, por lo que podemos actualizar sus valores (los valores de los parámetros, es decir). Para eso es necesario el argumento `require_grad = True`. Le dice a PyTorch que queremos que calcule gradientes para nosotros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.8096], requires_grad=True) tensor([1.5162], requires_grad=True)\n",
      "tensor([-0.9111], requires_grad=True) tensor([-0.6629], requires_grad=True)\n",
      "tensor([-0.2315], requires_grad=True) tensor([-0.5364], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# PRIMERO\n",
    "# Inicializa los parámetros \"a\" y \"b\" al azar, casi como lo hicimos en Numpy\n",
    "# ya que queremos aplicar descenso de gradiente en estos parámetros, necesitamos\n",
    "# establecer REQUIRES_GRAD = TRUE\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "print(a, b)\n",
    "\n",
    "# SEGUNDO\n",
    "# Pero, ¿qué pasa si queremos ejecutarlo en una GPU? Podríamos enviarlos al dispositivo, ¿verdad?\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float).to(dispositivo)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float).to(dispositivo)\n",
    "print(a, b)\n",
    "# ¡Lo siento, pero no! El (dispositivo) \"oculta\" el gradiente ...\n",
    "\n",
    "# TERCERO\n",
    "#Podemos crear tensores regulares y enviarlos al dispositivo (como hicimos con nuestros datos)\n",
    "a = torch.randn(1, dtype=torch.float).to(dispositivo)\n",
    "b = torch.randn(1, dtype=torch.float).to(dispositivo)\n",
    "# y ENTONCES los fijamos como que requieren gradientes ...\n",
    "a.requires_grad_()\n",
    "b.requires_grad_()\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primer fragmento de código crea dos tensores para nuestros parámetros, gradientes y todo. Pero son tensores de CPU.\n",
    "\n",
    "> PRIMERO  \n",
    "> tensor(\\[0.5771\\], requires_grad=True)  \n",
    "> tensor(\\[1.6813\\], requires_grad=True)\n",
    "\n",
    "En el segundo fragmento de código, probamos el enfoque ingenuo de enviarlos a nuestra GPU. Logramos enviarlos a otro dispositivo, pero de alguna manera \"perdimos\" los gradientes ...\n",
    "\n",
    "> SEGUNDO  \n",
    "> tensor(\\[0.5158\\], device='cuda:0', grad_fn=\\<CopyBackwards\\>)    \n",
    "> tensor(\\[0.0246\\], device='cuda:0', grad_fn=\\<CopyBackwards\\>)\n",
    "\n",
    "En el tercer fragmento, primero enviamos nuestros tensores al dispositivo y luego usamos el método `require_grad_()` para establecer su require_grad en True en su lugar.\n",
    "\n",
    "> TERCERO  \n",
    "> tensor(\\[0.5771\\], device='cuda:0', requires_grad=True)  \n",
    "> tensor(\\[1.6813\\], device='cuda:0', requires_grad=True)\n",
    "\n",
    "*En PyTorch, cada método que termina con un guión bajo (_) realiza cambios in situ, lo que significa que modificarán la variable subyacente.*\n",
    "\n",
    "Aunque el último enfoque funcionó bien, es mucho mejor asignar tensores a un dispositivo en el momento de su creación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3367], requires_grad=True) tensor([0.1288], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Podemos especificar el dispositivo en el momento de la creación. ¡RECOMENDADO!\n",
    "torch.manual_seed(42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=dispositivo)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=dispositivo)\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd\n",
    "\n",
    "Autograd es el paquete de diferenciación automática de PyTorch. Gracias a ello, no necesitamos preocuparnos por las derivadas parciales, regla de la cadena ni nada por el estilo.\n",
    "\n",
    "Entonces, ¿cómo le decimos a PyTorch que haga lo suyo y calcule todos los gradientes? Para eso sirve `backward()`.\n",
    "\n",
    "¿Recuerdas el punto de partida para calcular los gradientes? Fue la pérdida, ya que calculamos sus derivadas parciales con respecto a nuestros parámetros. Por lo tanto, debemos invocar el método `backward()` desde la variable Python correspondiente, como `perdida.backward()`.\n",
    "\n",
    "¿Qué pasa con los valores reales de los gradientes? Podemos inspeccionarlos mirando el atributo `grad` de un tensor.\n",
    "\n",
    "Si revisa la documentación del método, indica claramente que los gradientes se acumulan. Entonces, cada vez que usamos los gradientes para actualizar los parámetros, necesitamos poner a cero los gradientes después. Y para eso sirve `zero_()`.\n",
    "\n",
    "Entonces, abandonemos el cálculo manual de gradientes y usemos los métodos `backward()` y `zero_()` en su lugar.\n",
    "\n",
    "¿Eso es? Bueno, más o menos ... pero siempre hay un problema, y esta vez tiene que ver con la actualización de los parámetros ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-3.1125])\n",
      "tensor([-1.8156])\n",
      "tensor([-2.3184])\n",
      "tensor([-1.4064])\n",
      "tensor([-1.7219])\n",
      "tensor([-1.0982])\n",
      "tensor([-1.2737])\n",
      "tensor([-0.8659])\n",
      "tensor([-0.9372])\n",
      "tensor([-0.6906])\n",
      "tensor([-0.6845])\n",
      "tensor([-0.5583])\n",
      "tensor([-0.4948])\n",
      "tensor([-0.4582])\n",
      "tensor([-0.3526])\n",
      "tensor([-0.3824])\n",
      "tensor([-0.2459])\n",
      "tensor([-0.3248])\n",
      "tensor([-0.1660])\n",
      "tensor([-0.2810])\n",
      "tensor([-0.1063])\n",
      "tensor([-0.2475])\n",
      "tensor([-0.0616])\n",
      "tensor([-0.2218])\n",
      "tensor([-0.0283])\n",
      "tensor([-0.2019])\n",
      "tensor([-0.0036])\n",
      "tensor([-0.1864])\n",
      "tensor([0.0147])\n",
      "tensor([-0.1743])\n",
      "tensor([0.0283])\n",
      "tensor([-0.1646])\n",
      "tensor([0.0382])\n",
      "tensor([-0.1568])\n",
      "tensor([0.0453])\n",
      "tensor([-0.1505])\n",
      "tensor([0.0505])\n",
      "tensor([-0.1452])\n",
      "tensor([0.0541])\n",
      "tensor([-0.1408])\n",
      "tensor([0.0566])\n",
      "tensor([-0.1370])\n",
      "tensor([0.0582])\n",
      "tensor([-0.1337])\n",
      "tensor([0.0592])\n",
      "tensor([-0.1307])\n",
      "tensor([0.0597])\n",
      "tensor([-0.1280])\n",
      "tensor([0.0599])\n",
      "tensor([-0.1255])\n",
      "tensor([0.0598])\n",
      "tensor([-0.1232])\n",
      "tensor([0.0594])\n",
      "tensor([-0.1211])\n",
      "tensor([0.0590])\n",
      "tensor([-0.1190])\n",
      "tensor([0.0584])\n",
      "tensor([-0.1170])\n",
      "tensor([0.0578])\n",
      "tensor([-0.1151])\n",
      "tensor([0.0571])\n",
      "tensor([-0.1133])\n",
      "tensor([0.0564])\n",
      "tensor([-0.1115])\n",
      "tensor([0.0557])\n",
      "tensor([-0.1098])\n",
      "tensor([0.0549])\n",
      "tensor([-0.1081])\n",
      "tensor([0.0541])\n",
      "tensor([-0.1064])\n",
      "tensor([0.0534])\n",
      "tensor([-0.1048])\n",
      "tensor([0.0526])\n",
      "tensor([-0.1032])\n",
      "tensor([0.0518])\n",
      "tensor([-0.1016])\n",
      "tensor([0.0511])\n",
      "tensor([-0.1001])\n",
      "tensor([0.0503])\n",
      "tensor([-0.0985])\n",
      "tensor([0.0496])\n",
      "tensor([-0.0970])\n",
      "tensor([0.0488])\n",
      "tensor([-0.0956])\n",
      "tensor([0.0481])\n",
      "tensor([-0.0941])\n",
      "tensor([0.0474])\n",
      "tensor([-0.0927])\n",
      "tensor([0.0466])\n",
      "tensor([-0.0913])\n",
      "tensor([0.0459])\n",
      "tensor([-0.0899])\n",
      "tensor([0.0453])\n",
      "tensor([-0.0886])\n",
      "tensor([0.0446])\n",
      "tensor([-0.0872])\n",
      "tensor([0.0439])\n",
      "tensor([-0.0859])\n",
      "tensor([0.0432])\n",
      "tensor([-0.0846])\n",
      "tensor([0.0426])\n",
      "tensor([-0.0833])\n",
      "tensor([0.0419])\n",
      "tensor([-0.0821])\n",
      "tensor([0.0413])\n",
      "tensor([-0.0808])\n",
      "tensor([0.0407])\n",
      "tensor([-0.0796])\n",
      "tensor([0.0401])\n",
      "tensor([-0.0784])\n",
      "tensor([0.0395])\n",
      "tensor([-0.0772])\n",
      "tensor([0.0389])\n",
      "tensor([-0.0761])\n",
      "tensor([0.0383])\n",
      "tensor([-0.0749])\n",
      "tensor([0.0377])\n",
      "tensor([-0.0738])\n",
      "tensor([0.0371])\n",
      "tensor([-0.0727])\n",
      "tensor([0.0366])\n",
      "tensor([-0.0716])\n",
      "tensor([0.0360])\n",
      "tensor([-0.0705])\n",
      "tensor([0.0355])\n",
      "tensor([-0.0694])\n",
      "tensor([0.0349])\n",
      "tensor([-0.0684])\n",
      "tensor([0.0344])\n",
      "tensor([-0.0673])\n",
      "tensor([0.0339])\n",
      "tensor([-0.0663])\n",
      "tensor([0.0334])\n",
      "tensor([-0.0653])\n",
      "tensor([0.0329])\n",
      "tensor([-0.0643])\n",
      "tensor([0.0324])\n",
      "tensor([-0.0634])\n",
      "tensor([0.0319])\n",
      "tensor([-0.0624])\n",
      "tensor([0.0314])\n",
      "tensor([-0.0615])\n",
      "tensor([0.0309])\n",
      "tensor([-0.0605])\n",
      "tensor([0.0305])\n",
      "tensor([-0.0596])\n",
      "tensor([0.0300])\n",
      "tensor([-0.0587])\n",
      "tensor([0.0296])\n",
      "tensor([-0.0578])\n",
      "tensor([0.0291])\n",
      "tensor([-0.0570])\n",
      "tensor([0.0287])\n",
      "tensor([-0.0561])\n",
      "tensor([0.0282])\n",
      "tensor([-0.0552])\n",
      "tensor([0.0278])\n",
      "tensor([-0.0544])\n",
      "tensor([0.0274])\n",
      "tensor([-0.0536])\n",
      "tensor([0.0270])\n",
      "tensor([-0.0528])\n",
      "tensor([0.0266])\n",
      "tensor([-0.0520])\n",
      "tensor([0.0262])\n",
      "tensor([-0.0512])\n",
      "tensor([0.0258])\n",
      "tensor([-0.0504])\n",
      "tensor([0.0254])\n",
      "tensor([-0.0497])\n",
      "tensor([0.0250])\n",
      "tensor([-0.0489])\n",
      "tensor([0.0246])\n",
      "tensor([-0.0482])\n",
      "tensor([0.0242])\n",
      "tensor([-0.0474])\n",
      "tensor([0.0239])\n",
      "tensor([-0.0467])\n",
      "tensor([0.0235])\n",
      "tensor([-0.0460])\n",
      "tensor([0.0232])\n",
      "tensor([-0.0453])\n",
      "tensor([0.0228])\n",
      "tensor([-0.0446])\n",
      "tensor([0.0225])\n",
      "tensor([-0.0440])\n",
      "tensor([0.0221])\n",
      "tensor([-0.0433])\n",
      "tensor([0.0218])\n",
      "tensor([-0.0426])\n",
      "tensor([0.0215])\n",
      "tensor([-0.0420])\n",
      "tensor([0.0211])\n",
      "tensor([-0.0414])\n",
      "tensor([0.0208])\n",
      "tensor([-0.0407])\n",
      "tensor([0.0205])\n",
      "tensor([-0.0401])\n",
      "tensor([0.0202])\n",
      "tensor([-0.0395])\n",
      "tensor([0.0199])\n",
      "tensor([-0.0389])\n",
      "tensor([0.0196])\n",
      "tensor([-0.0383])\n",
      "tensor([0.0193])\n",
      "tensor([-0.0378])\n",
      "tensor([0.0190])\n",
      "tensor([-0.0372])\n",
      "tensor([0.0187])\n",
      "tensor([-0.0366])\n",
      "tensor([0.0184])\n",
      "tensor([-0.0361])\n",
      "tensor([0.0182])\n",
      "tensor([-0.0355])\n",
      "tensor([0.0179])\n",
      "tensor([-0.0350])\n",
      "tensor([0.0176])\n",
      "tensor([-0.0345])\n",
      "tensor([0.0173])\n",
      "tensor([-0.0339])\n",
      "tensor([0.0171])\n",
      "tensor([-0.0334])\n",
      "tensor([0.0168])\n",
      "tensor([-0.0329])\n",
      "tensor([0.0166])\n",
      "tensor([-0.0324])\n",
      "tensor([0.0163])\n",
      "tensor([-0.0319])\n",
      "tensor([0.0161])\n",
      "tensor([-0.0315])\n",
      "tensor([0.0158])\n",
      "tensor([-0.0310])\n",
      "tensor([0.0156])\n",
      "tensor([-0.0305])\n",
      "tensor([0.0154])\n",
      "tensor([-0.0301])\n",
      "tensor([0.0151])\n",
      "tensor([-0.0296])\n",
      "tensor([0.0149])\n",
      "tensor([-0.0291])\n",
      "tensor([0.0147])\n",
      "tensor([-0.0287])\n",
      "tensor([0.0145])\n",
      "tensor([-0.0283])\n",
      "tensor([0.0142])\n",
      "tensor([-0.0278])\n",
      "tensor([0.0140])\n",
      "tensor([-0.0274])\n",
      "tensor([0.0138])\n",
      "tensor([-0.0270])\n",
      "tensor([0.0136])\n",
      "tensor([-0.0266])\n",
      "tensor([0.0134])\n",
      "tensor([-0.0262])\n",
      "tensor([0.0132])\n",
      "tensor([-0.0258])\n",
      "tensor([0.0130])\n",
      "tensor([-0.0254])\n",
      "tensor([0.0128])\n",
      "tensor([-0.0250])\n",
      "tensor([0.0126])\n",
      "tensor([-0.0247])\n",
      "tensor([0.0124])\n",
      "tensor([-0.0243])\n",
      "tensor([0.0122])\n",
      "tensor([-0.0239])\n",
      "tensor([0.0120])\n",
      "tensor([-0.0236])\n",
      "tensor([0.0119])\n",
      "tensor([-0.0232])\n",
      "tensor([0.0117])\n",
      "tensor([-0.0228])\n",
      "tensor([0.0115])\n",
      "tensor([-0.0225])\n",
      "tensor([0.0113])\n",
      "tensor([-0.0222])\n",
      "tensor([0.0112])\n",
      "tensor([-0.0218])\n",
      "tensor([0.0110])\n",
      "tensor([-0.0215])\n",
      "tensor([0.0108])\n",
      "tensor([-0.0212])\n",
      "tensor([0.0107])\n",
      "tensor([-0.0209])\n",
      "tensor([0.0105])\n",
      "tensor([-0.0205])\n",
      "tensor([0.0103])\n",
      "tensor([-0.0202])\n",
      "tensor([0.0102])\n",
      "tensor([-0.0199])\n",
      "tensor([0.0100])\n",
      "tensor([-0.0196])\n",
      "tensor([0.0099])\n",
      "tensor([-0.0193])\n",
      "tensor([0.0097])\n",
      "tensor([-0.0190])\n",
      "tensor([0.0096])\n",
      "tensor([-0.0187])\n",
      "tensor([0.0094])\n",
      "tensor([-0.0185])\n",
      "tensor([0.0093])\n",
      "tensor([-0.0182])\n",
      "tensor([0.0092])\n",
      "tensor([-0.0179])\n",
      "tensor([0.0090])\n",
      "tensor([-0.0176])\n",
      "tensor([0.0089])\n",
      "tensor([-0.0174])\n",
      "tensor([0.0087])\n",
      "tensor([-0.0171])\n",
      "tensor([0.0086])\n",
      "tensor([-0.0169])\n",
      "tensor([0.0085])\n",
      "tensor([-0.0166])\n",
      "tensor([0.0084])\n",
      "tensor([-0.0163])\n",
      "tensor([0.0082])\n",
      "tensor([-0.0161])\n",
      "tensor([0.0081])\n",
      "tensor([-0.0159])\n",
      "tensor([0.0080])\n",
      "tensor([-0.0156])\n",
      "tensor([0.0079])\n",
      "tensor([-0.0154])\n",
      "tensor([0.0077])\n",
      "tensor([-0.0151])\n",
      "tensor([0.0076])\n",
      "tensor([-0.0149])\n",
      "tensor([0.0075])\n",
      "tensor([-0.0147])\n",
      "tensor([0.0074])\n",
      "tensor([-0.0145])\n",
      "tensor([0.0073])\n",
      "tensor([-0.0143])\n",
      "tensor([0.0072])\n",
      "tensor([-0.0140])\n",
      "tensor([0.0071])\n",
      "tensor([-0.0138])\n",
      "tensor([0.0070])\n",
      "tensor([-0.0136])\n",
      "tensor([0.0069])\n",
      "tensor([-0.0134])\n",
      "tensor([0.0068])\n",
      "tensor([-0.0132])\n",
      "tensor([0.0066])\n",
      "tensor([-0.0130])\n",
      "tensor([0.0065])\n",
      "tensor([-0.0128])\n",
      "tensor([0.0064])\n",
      "tensor([-0.0126])\n",
      "tensor([0.0064])\n",
      "tensor([-0.0124])\n",
      "tensor([0.0063])\n",
      "tensor([-0.0122])\n",
      "tensor([0.0062])\n",
      "tensor([-0.0121])\n",
      "tensor([0.0061])\n",
      "tensor([-0.0119])\n",
      "tensor([0.0060])\n",
      "tensor([-0.0117])\n",
      "tensor([0.0059])\n",
      "tensor([-0.0115])\n",
      "tensor([0.0058])\n",
      "tensor([-0.0113])\n",
      "tensor([0.0057])\n",
      "tensor([-0.0112])\n",
      "tensor([0.0056])\n",
      "tensor([-0.0110])\n",
      "tensor([0.0055])\n",
      "tensor([-0.0108])\n",
      "tensor([0.0055])\n",
      "tensor([-0.0107])\n",
      "tensor([0.0054])\n",
      "tensor([-0.0105])\n",
      "tensor([0.0053])\n",
      "tensor([-0.0104])\n",
      "tensor([0.0052])\n",
      "tensor([-0.0102])\n",
      "tensor([0.0051])\n",
      "tensor([-0.0100])\n",
      "tensor([0.0051])\n",
      "tensor([-0.0099])\n",
      "tensor([0.0050])\n",
      "tensor([-0.0097])\n",
      "tensor([0.0049])\n",
      "tensor([-0.0096])\n",
      "tensor([0.0048])\n",
      "tensor([-0.0094])\n",
      "tensor([0.0048])\n",
      "tensor([-0.0093])\n",
      "tensor([0.0047])\n",
      "tensor([-0.0092])\n",
      "tensor([0.0046])\n",
      "tensor([-0.0090])\n",
      "tensor([0.0045])\n",
      "tensor([-0.0089])\n",
      "tensor([0.0045])\n",
      "tensor([-0.0088])\n",
      "tensor([0.0044])\n",
      "tensor([-0.0086])\n",
      "tensor([0.0043])\n",
      "tensor([-0.0085])\n",
      "tensor([0.0043])\n",
      "tensor([-0.0084])\n",
      "tensor([0.0042])\n",
      "tensor([-0.0082])\n",
      "tensor([0.0041])\n",
      "tensor([-0.0081])\n",
      "tensor([0.0041])\n",
      "tensor([-0.0080])\n",
      "tensor([0.0040])\n",
      "tensor([-0.0079])\n",
      "tensor([0.0040])\n",
      "tensor([-0.0078])\n",
      "tensor([0.0039])\n",
      "tensor([-0.0076])\n",
      "tensor([0.0038])\n",
      "tensor([-0.0075])\n",
      "tensor([0.0038])\n",
      "tensor([-0.0074])\n",
      "tensor([0.0037])\n",
      "tensor([-0.0073])\n",
      "tensor([0.0037])\n",
      "tensor([-0.0072])\n",
      "tensor([0.0036])\n",
      "tensor([-0.0071])\n",
      "tensor([0.0036])\n",
      "tensor([-0.0070])\n",
      "tensor([0.0035])\n",
      "tensor([-0.0069])\n",
      "tensor([0.0035])\n",
      "tensor([-0.0068])\n",
      "tensor([0.0034])\n",
      "tensor([-0.0067])\n",
      "tensor([0.0034])\n",
      "tensor([-0.0066])\n",
      "tensor([0.0033])\n",
      "tensor([-0.0065])\n",
      "tensor([0.0033])\n",
      "tensor([-0.0064])\n",
      "tensor([0.0032])\n",
      "tensor([-0.0063])\n",
      "tensor([0.0032])\n",
      "tensor([-0.0062])\n",
      "tensor([0.0031])\n",
      "tensor([-0.0061])\n",
      "tensor([0.0031])\n",
      "tensor([-0.0060])\n",
      "tensor([0.0030])\n",
      "tensor([-0.0059])\n",
      "tensor([0.0030])\n",
      "tensor([-0.0058])\n",
      "tensor([0.0029])\n",
      "tensor([-0.0057])\n",
      "tensor([0.0029])\n",
      "tensor([-0.0056])\n",
      "tensor([0.0028])\n",
      "tensor([-0.0055])\n",
      "tensor([0.0028])\n",
      "tensor([-0.0055])\n",
      "tensor([0.0027])\n",
      "tensor([-0.0054])\n",
      "tensor([0.0027])\n",
      "tensor([-0.0053])\n",
      "tensor([0.0027])\n",
      "tensor([-0.0052])\n",
      "tensor([0.0026])\n",
      "tensor([-0.0051])\n",
      "tensor([0.0026])\n",
      "tensor([-0.0051])\n",
      "tensor([0.0025])\n",
      "tensor([-0.0050])\n",
      "tensor([0.0025])\n",
      "tensor([-0.0049])\n",
      "tensor([0.0025])\n",
      "tensor([-0.0048])\n",
      "tensor([0.0024])\n",
      "tensor([-0.0048])\n",
      "tensor([0.0024])\n",
      "tensor([-0.0047])\n",
      "tensor([0.0024])\n",
      "tensor([-0.0046])\n",
      "tensor([0.0023])\n",
      "tensor([-0.0046])\n",
      "tensor([0.0023])\n",
      "tensor([-0.0045])\n",
      "tensor([0.0023])\n",
      "tensor([-0.0044])\n",
      "tensor([0.0022])\n",
      "tensor([-0.0043])\n",
      "tensor([0.0022])\n",
      "tensor([-0.0043])\n",
      "tensor([0.0022])\n",
      "tensor([-0.0042])\n",
      "tensor([0.0021])\n",
      "tensor([-0.0042])\n",
      "tensor([0.0021])\n",
      "tensor([-0.0041])\n",
      "tensor([0.0021])\n",
      "tensor([-0.0040])\n",
      "tensor([0.0020])\n",
      "tensor([-0.0040])\n",
      "tensor([0.0020])\n",
      "tensor([-0.0039])\n",
      "tensor([0.0020])\n",
      "tensor([-0.0038])\n",
      "tensor([0.0019])\n",
      "tensor([-0.0038])\n",
      "tensor([0.0019])\n",
      "tensor([-0.0037])\n",
      "tensor([0.0019])\n",
      "tensor([-0.0037])\n",
      "tensor([0.0019])\n",
      "tensor([-0.0036])\n",
      "tensor([0.0018])\n",
      "tensor([-0.0036])\n",
      "tensor([0.0018])\n",
      "tensor([-0.0035])\n",
      "tensor([0.0018])\n",
      "tensor([-0.0035])\n",
      "tensor([0.0017])\n",
      "tensor([-0.0034])\n",
      "tensor([0.0017])\n",
      "tensor([-0.0034])\n",
      "tensor([0.0017])\n",
      "tensor([-0.0033])\n",
      "tensor([0.0017])\n",
      "tensor([-0.0033])\n",
      "tensor([0.0016])\n",
      "tensor([-0.0032])\n",
      "tensor([0.0016])\n",
      "tensor([-0.0032])\n",
      "tensor([0.0016])\n",
      "tensor([-0.0031])\n",
      "tensor([0.0016])\n",
      "tensor([-0.0031])\n",
      "tensor([0.0015])\n",
      "tensor([-0.0030])\n",
      "tensor([0.0015])\n",
      "tensor([-0.0030])\n",
      "tensor([0.0015])\n",
      "tensor([-0.0029])\n",
      "tensor([0.0015])\n",
      "tensor([-0.0029])\n",
      "tensor([0.0015])\n",
      "tensor([-0.0028])\n",
      "tensor([0.0014])\n",
      "tensor([-0.0028])\n",
      "tensor([0.0014])\n",
      "tensor([-0.0028])\n",
      "tensor([0.0014])\n",
      "tensor([-0.0027])\n",
      "tensor([0.0014])\n",
      "tensor([-0.0027])\n",
      "tensor([0.0013])\n",
      "tensor([-0.0026])\n",
      "tensor([0.0013])\n",
      "tensor([-0.0026])\n",
      "tensor([0.0013])\n",
      "tensor([-0.0026])\n",
      "tensor([0.0013])\n",
      "tensor([-0.0025])\n",
      "tensor([0.0013])\n",
      "tensor([-0.0025])\n",
      "tensor([0.0012])\n",
      "tensor([-0.0024])\n",
      "tensor([0.0012])\n",
      "tensor([-0.0024])\n",
      "tensor([0.0012])\n",
      "tensor([-0.0024])\n",
      "tensor([0.0012])\n",
      "tensor([-0.0023])\n",
      "tensor([0.0012])\n",
      "tensor([-0.0023])\n",
      "tensor([0.0012])\n",
      "tensor([-0.0023])\n",
      "tensor([0.0011])\n",
      "tensor([-0.0022])\n",
      "tensor([0.0011])\n",
      "tensor([-0.0022])\n",
      "tensor([0.0011])\n",
      "tensor([-0.0022])\n",
      "tensor([0.0011])\n",
      "tensor([-0.0021])\n",
      "tensor([0.0011])\n",
      "tensor([-0.0021])\n",
      "tensor([0.0011])\n",
      "tensor([-0.0021])\n",
      "tensor([0.0010])\n",
      "tensor([-0.0020])\n",
      "tensor([0.0010])\n",
      "tensor([-0.0020])\n",
      "tensor([0.0010])\n",
      "tensor([-0.0020])\n",
      "tensor([0.0010])\n",
      "tensor([-0.0019])\n",
      "tensor([0.0010])\n",
      "tensor([-0.0019])\n",
      "tensor([0.0010])\n",
      "tensor([-0.0019])\n",
      "tensor([0.0009])\n",
      "tensor([-0.0019])\n",
      "tensor([0.0009])\n",
      "tensor([-0.0018])\n",
      "tensor([0.0009])\n",
      "tensor([-0.0018])\n",
      "tensor([0.0009])\n",
      "tensor([-0.0018])\n",
      "tensor([0.0009])\n",
      "tensor([-0.0017])\n",
      "tensor([0.0009])\n",
      "tensor([-0.0017])\n",
      "tensor([0.0009])\n",
      "tensor([-0.0017])\n",
      "tensor([0.0009])\n",
      "tensor([-0.0017])\n",
      "tensor([0.0008])\n",
      "tensor([-0.0016])\n",
      "tensor([0.0008])\n",
      "tensor([-0.0016])\n",
      "tensor([0.0008])\n",
      "tensor([-0.0016])\n",
      "tensor([0.0008])\n",
      "tensor([-0.0016])\n",
      "tensor([0.0008])\n",
      "tensor([-0.0015])\n",
      "tensor([0.0008])\n",
      "tensor([-0.0015])\n",
      "tensor([0.0008])\n",
      "tensor([-0.0015])\n",
      "tensor([0.0008])\n",
      "tensor([-0.0015])\n",
      "tensor([0.0007])\n",
      "tensor([-0.0015])\n",
      "tensor([0.0007])\n",
      "tensor([-0.0014])\n",
      "tensor([0.0007])\n",
      "tensor([-0.0014])\n",
      "tensor([0.0007])\n",
      "tensor([-0.0014])\n",
      "tensor([0.0007])\n",
      "tensor([-0.0014])\n",
      "tensor([0.0007])\n",
      "tensor([-0.0013])\n",
      "tensor([0.0007])\n",
      "tensor([-0.0013])\n",
      "tensor([0.0007])\n",
      "tensor([-0.0013])\n",
      "tensor([0.0007])\n",
      "tensor([-0.0013])\n",
      "tensor([0.0006])\n",
      "tensor([-0.0013])\n",
      "tensor([0.0006])\n",
      "tensor([-0.0012])\n",
      "tensor([0.0006])\n",
      "tensor([-0.0012])\n",
      "tensor([0.0006])\n",
      "tensor([-0.0012])\n",
      "tensor([0.0006])\n",
      "tensor([-0.0012])\n",
      "tensor([0.0006])\n",
      "tensor([-0.0012])\n",
      "tensor([0.0006])\n",
      "tensor([-0.0012])\n",
      "tensor([0.0006])\n",
      "tensor([-0.0011])\n",
      "tensor([0.0006])\n",
      "tensor([-0.0011])\n",
      "tensor([0.0006])\n",
      "tensor([-0.0011])\n",
      "tensor([0.0006])\n",
      "tensor([-0.0011])\n",
      "tensor([0.0005])\n",
      "tensor([-0.0011])\n",
      "tensor([0.0005])\n",
      "tensor([-0.0011])\n",
      "tensor([0.0005])\n",
      "tensor([-0.0010])\n",
      "tensor([0.0005])\n",
      "tensor([-0.0010])\n",
      "tensor([0.0005])\n",
      "tensor([-0.0010])\n",
      "tensor([0.0005])\n",
      "tensor([-0.0010])\n",
      "tensor([0.0005])\n",
      "tensor([-0.0010])\n",
      "tensor([0.0005])\n",
      "tensor([-0.0010])\n",
      "tensor([0.0005])\n",
      "tensor([-0.0009])\n",
      "tensor([0.0005])\n",
      "tensor([-0.0009])\n",
      "tensor([0.0005])\n",
      "tensor([-0.0009])\n",
      "tensor([0.0005])\n",
      "tensor([-0.0009])\n",
      "tensor([0.0005])\n",
      "tensor([-0.0009])\n",
      "tensor([0.0004])\n",
      "tensor([-0.0009])\n",
      "tensor([0.0004])\n",
      "tensor([-0.0009])\n",
      "tensor([0.0004])\n",
      "tensor([-0.0009])\n",
      "tensor([0.0004])\n",
      "tensor([-0.0008])\n",
      "tensor([0.0004])\n",
      "tensor([-0.0008])\n",
      "tensor([0.0004])\n",
      "tensor([-0.0008])\n",
      "tensor([0.0004])\n",
      "tensor([-0.0008])\n",
      "tensor([0.0004])\n",
      "tensor([-0.0008])\n",
      "tensor([0.0004])\n",
      "tensor([-0.0008])\n",
      "tensor([0.0004])\n",
      "tensor([-0.0008])\n",
      "tensor([0.0004])\n",
      "tensor([-0.0008])\n",
      "tensor([0.0004])\n",
      "tensor([-0.0007])\n",
      "tensor([0.0004])\n",
      "tensor([-0.0007])\n",
      "tensor([0.0004])\n",
      "tensor([-0.0007])\n",
      "tensor([0.0004])\n",
      "tensor([-0.0007])\n",
      "tensor([0.0004])\n",
      "tensor([-0.0007])\n",
      "tensor([0.0004])\n",
      "tensor([-0.0007])\n",
      "tensor([0.0003])\n",
      "tensor([-0.0007])\n",
      "tensor([0.0003])\n",
      "tensor([-0.0007])\n",
      "tensor([0.0003])\n",
      "tensor([-0.0007])\n",
      "tensor([0.0003])\n",
      "tensor([-0.0006])\n",
      "tensor([0.0003])\n",
      "tensor([-0.0006])\n",
      "tensor([0.0003])\n",
      "tensor([-0.0006])\n",
      "tensor([0.0003])\n",
      "tensor([-0.0006])\n",
      "tensor([0.0003])\n",
      "tensor([-0.0006])\n",
      "tensor([0.0003])\n",
      "tensor([-0.0006])\n",
      "tensor([0.0003])\n",
      "tensor([-0.0006])\n",
      "tensor([0.0003])\n",
      "tensor([-0.0006])\n",
      "tensor([0.0003])\n",
      "tensor([-0.0006])\n",
      "tensor([0.0003])\n",
      "tensor([-0.0006])\n",
      "tensor([0.0003])\n",
      "tensor([-0.0006])\n",
      "tensor([0.0003])\n",
      "tensor([-0.0005])\n",
      "tensor([0.0003])\n",
      "tensor([-0.0005])\n",
      "tensor([0.0003])\n",
      "tensor([-0.0005])\n",
      "tensor([0.0003])\n",
      "tensor([-0.0005])\n",
      "tensor([0.0003])\n",
      "tensor([-0.0005])\n",
      "tensor([0.0003])\n",
      "tensor([-0.0005])\n",
      "tensor([0.0003])\n",
      "tensor([-0.0005])\n",
      "tensor([0.0003])\n",
      "tensor([-0.0005])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0005])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0005])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0005])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0005])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0005])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0005])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0004])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0004])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0004])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0004])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0004])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0004])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0004])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0004])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0004])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0004])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0004])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0004])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0004])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0004])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0004])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0004])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0003])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0003])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0003])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0003])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0003])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0003])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0003])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0003])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0003])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0003])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0003])\n",
      "tensor([0.0002])\n",
      "tensor([-0.0003])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0003])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0003])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0003])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0003])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0003])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0003])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0003])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0003])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0003])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0003])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0002])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0002])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0002])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0002])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0002])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0002])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0002])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0002])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0002])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0002])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0002])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0002])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0002])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0002])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0002])\n",
      "tensor([0.0001])\n",
      "tensor([-0.0002])\n",
      "tensor([9.9594e-05])\n",
      "tensor([-0.0002])\n",
      "tensor([9.7900e-05])\n",
      "tensor([-0.0002])\n",
      "tensor([9.6527e-05])\n",
      "tensor([-0.0002])\n",
      "tensor([9.5085e-05])\n",
      "tensor([-0.0002])\n",
      "tensor([9.3636e-05])\n",
      "tensor([-0.0002])\n",
      "tensor([9.2130e-05])\n",
      "tensor([-0.0002])\n",
      "tensor([9.0872e-05])\n",
      "tensor([-0.0002])\n",
      "tensor([8.9555e-05])\n",
      "tensor([-0.0002])\n",
      "tensor([8.8250e-05])\n",
      "tensor([-0.0002])\n",
      "tensor([8.6918e-05])\n",
      "tensor([-0.0002])\n",
      "tensor([8.5517e-05])\n",
      "tensor([-0.0002])\n",
      "tensor([8.4095e-05])\n",
      "tensor([-0.0002])\n",
      "tensor([8.2712e-05])\n",
      "tensor([-0.0002])\n",
      "tensor([8.1570e-05])\n",
      "tensor([-0.0002])\n",
      "tensor([8.0486e-05])\n",
      "tensor([-0.0002])\n",
      "tensor([7.9145e-05])\n",
      "tensor([-0.0002])\n",
      "tensor([7.8051e-05])\n",
      "tensor([-0.0002])\n",
      "tensor([7.6955e-05])\n",
      "tensor([-0.0002])\n",
      "tensor([7.5681e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([7.4613e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([7.3319e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([7.2196e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([7.1032e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([6.9903e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([6.8819e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([6.7823e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([6.6842e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([6.5867e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([6.4933e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([6.4093e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([6.3060e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([6.2128e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([6.1231e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([6.0351e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([5.9320e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([5.8340e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([5.7453e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([5.6731e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([5.5748e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([5.4913e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([5.4107e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([5.3402e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([5.2584e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([5.1776e-05])\n",
      "tensor([-0.0001])\n",
      "tensor([5.1087e-05])\n",
      "tensor([-9.9639e-05])\n",
      "tensor([5.0304e-05])\n",
      "tensor([-9.8126e-05])\n",
      "tensor([4.9497e-05])\n",
      "tensor([-9.6684e-05])\n",
      "tensor([4.8653e-05])\n",
      "tensor([-9.5258e-05])\n",
      "tensor([4.7852e-05])\n",
      "tensor([-9.3863e-05])\n",
      "tensor([4.7217e-05])\n",
      "tensor([-9.2393e-05])\n",
      "tensor([4.6496e-05])\n",
      "tensor([-9.0986e-05])\n",
      "tensor([4.5731e-05])\n",
      "tensor([-8.9653e-05])\n",
      "tensor([4.5108e-05])\n",
      "tensor([-8.8273e-05])\n",
      "tensor([4.4387e-05])\n",
      "tensor([-8.6958e-05])\n",
      "tensor([4.3814e-05])\n",
      "tensor([-8.5594e-05])\n",
      "tensor([4.3082e-05])\n",
      "tensor([-8.4333e-05])\n",
      "tensor([4.2502e-05])\n",
      "tensor([-8.3017e-05])\n",
      "tensor([4.1800e-05])\n",
      "tensor([-8.1784e-05])\n",
      "tensor([4.1258e-05])\n",
      "tensor([-8.0489e-05])\n",
      "tensor([4.0558e-05])\n",
      "tensor([-7.9301e-05])\n",
      "tensor([3.9983e-05])\n",
      "tensor([-7.8079e-05])\n",
      "tensor([3.9203e-05])\n",
      "tensor([-7.6996e-05])\n",
      "tensor([3.8651e-05])\n",
      "tensor([-7.5801e-05])\n",
      "tensor([3.8224e-05])\n",
      "tensor([-7.4577e-05])\n",
      "tensor([3.7707e-05])\n",
      "tensor([-7.3411e-05])\n",
      "tensor([3.7103e-05])\n",
      "tensor([-7.2301e-05])\n",
      "tensor([3.6565e-05])\n",
      "tensor([-7.1188e-05])\n",
      "tensor([3.5924e-05])\n",
      "tensor([-7.0153e-05])\n",
      "tensor([3.5397e-05])\n",
      "tensor([-6.9097e-05])\n",
      "tensor([3.4793e-05])\n",
      "tensor([-6.8070e-05])\n",
      "tensor([3.4280e-05])\n",
      "tensor([-6.7045e-05])\n",
      "tensor([3.3689e-05])\n",
      "tensor([-6.6071e-05])\n",
      "tensor([3.3202e-05])\n",
      "tensor([-6.5076e-05])\n",
      "tensor([3.2708e-05])\n",
      "tensor([-6.4070e-05])\n",
      "tensor([3.2351e-05])\n",
      "tensor([-6.3039e-05])\n",
      "tensor([3.1894e-05])\n",
      "tensor([-6.2071e-05])\n",
      "tensor([3.1338e-05])\n",
      "tensor([-6.1164e-05])\n",
      "tensor([3.0884e-05])\n",
      "tensor([-6.0245e-05])\n",
      "tensor([3.0420e-05])\n",
      "tensor([-5.9323e-05])\n",
      "tensor([2.9829e-05])\n",
      "tensor([-5.8492e-05])\n",
      "tensor([2.9391e-05])\n",
      "tensor([-5.7597e-05])\n",
      "tensor([2.8837e-05])\n",
      "tensor([-5.6787e-05])\n",
      "tensor([2.8545e-05])\n",
      "tensor([-5.5844e-05])\n",
      "tensor([2.8068e-05])\n",
      "tensor([-5.5038e-05])\n",
      "tensor([2.7576e-05])\n",
      "tensor([-5.4227e-05])\n",
      "tensor([2.7157e-05])\n",
      "tensor([-5.3417e-05])\n",
      "tensor([2.6736e-05])\n",
      "tensor([-5.2609e-05])\n",
      "tensor([2.6422e-05])\n",
      "tensor([-5.1769e-05])\n",
      "tensor([2.6061e-05])\n",
      "tensor([-5.0980e-05])\n",
      "tensor([2.5658e-05])\n",
      "tensor([-5.0211e-05])\n",
      "tensor([2.5140e-05])\n",
      "tensor([-4.9515e-05])\n",
      "tensor([2.4863e-05])\n",
      "tensor([-4.8710e-05])\n",
      "tensor([2.4470e-05])\n",
      "tensor([-4.7986e-05])\n",
      "tensor([2.3977e-05])\n",
      "tensor([-4.7319e-05])\n",
      "tensor([2.3683e-05])\n",
      "tensor([-4.6571e-05])\n",
      "tensor([2.3326e-05])\n",
      "tensor([-4.5855e-05])\n",
      "tensor([2.2854e-05])\n",
      "tensor([-4.5239e-05])\n",
      "tensor([2.2599e-05])\n",
      "tensor([-4.4503e-05])\n",
      "tensor([2.2242e-05])\n",
      "tensor([-4.3847e-05])\n",
      "tensor([2.1889e-05])\n",
      "tensor([-4.3183e-05])\n",
      "tensor([2.1613e-05])\n",
      "tensor([-4.2511e-05])\n",
      "tensor([2.1397e-05])\n",
      "tensor([-4.1805e-05])\n",
      "tensor([2.1038e-05])\n",
      "tensor([-4.1201e-05])\n",
      "tensor([2.0693e-05])\n",
      "tensor([-4.0582e-05])\n",
      "tensor([2.0460e-05])\n",
      "tensor([-3.9930e-05])\n",
      "tensor([2.0135e-05])\n",
      "tensor([-3.9340e-05])\n",
      "tensor([1.9830e-05])\n",
      "tensor([-3.8748e-05])\n",
      "tensor([1.9469e-05])\n",
      "tensor([-3.8179e-05])\n",
      "tensor([1.9224e-05])\n",
      "tensor([-3.7587e-05])\n",
      "tensor([1.9010e-05])\n",
      "tensor([-3.6972e-05])\n",
      "tensor([1.8727e-05])\n",
      "tensor([-3.6408e-05])\n",
      "tensor([1.8393e-05])\n",
      "tensor([-3.5875e-05])\n",
      "tensor([1.8167e-05])\n",
      "tensor([-3.5313e-05])\n",
      "tensor([1.7991e-05])\n",
      "tensor([-3.4717e-05])\n",
      "tensor([1.7695e-05])\n",
      "tensor([-3.4217e-05])\n",
      "tensor([1.7414e-05])\n",
      "tensor([-3.3695e-05])\n",
      "tensor([1.6962e-05])\n",
      "tensor([-3.3284e-05])\n",
      "tensor([1.6761e-05])\n",
      "tensor([-3.2756e-05])\n",
      "tensor([1.6481e-05])\n",
      "tensor([-3.2283e-05])\n",
      "tensor([1.6171e-05])\n",
      "tensor([-3.1830e-05])\n",
      "tensor([1.5909e-05])\n",
      "tensor([-3.1345e-05])\n",
      "tensor([1.5728e-05])\n",
      "tensor([-3.0850e-05])\n",
      "tensor([1.5565e-05])\n",
      "tensor([-3.0346e-05])\n",
      "tensor([1.5289e-05])\n",
      "tensor([-2.9915e-05])\n",
      "tensor([1.4985e-05])\n",
      "tensor([-2.9496e-05])\n",
      "tensor([1.4713e-05])\n",
      "tensor([-2.9066e-05])\n",
      "tensor([1.4536e-05])\n",
      "tensor([-2.8615e-05])\n",
      "tensor([1.4409e-05])\n",
      "tensor([-2.8138e-05])\n",
      "tensor([1.4251e-05])\n",
      "tensor([-2.7683e-05])\n",
      "tensor([1.3953e-05])\n",
      "tensor([-2.7311e-05])\n",
      "tensor([1.3675e-05])\n",
      "tensor([-2.6926e-05])\n",
      "tensor([1.3649e-05])\n",
      "tensor([-2.6419e-05])\n",
      "tensor([1.3523e-05])\n",
      "tensor([-2.5994e-05])\n",
      "tensor([1.3353e-05])\n",
      "tensor([-2.5577e-05])\n",
      "tensor([1.3141e-05])\n",
      "tensor([-2.5200e-05])\n",
      "tensor([1.2852e-05])\n",
      "tensor([-2.4875e-05])\n",
      "tensor([1.2607e-05])\n",
      "tensor([-2.4519e-05])\n",
      "tensor([1.2361e-05])\n",
      "tensor([-2.4166e-05])\n",
      "tensor([1.2211e-05])\n",
      "tensor([-2.3796e-05])\n",
      "tensor([1.2074e-05])\n",
      "tensor([-2.3412e-05])\n",
      "tensor([1.1940e-05])\n",
      "tensor([-2.3025e-05])\n",
      "tensor([1.1698e-05])\n",
      "tensor([-2.2717e-05])\n",
      "tensor([1.1454e-05])\n",
      "tensor([-2.2411e-05])\n",
      "tensor([1.1186e-05])\n",
      "tensor([-2.2111e-05])\n",
      "tensor([1.1191e-05])\n",
      "tensor([-2.1685e-05])\n",
      "tensor([1.1090e-05])\n",
      "tensor([-2.1328e-05])\n",
      "tensor([1.0991e-05])\n",
      "tensor([-2.0966e-05])\n",
      "tensor([1.0841e-05])\n",
      "tensor([-2.0638e-05])\n",
      "tensor([1.0597e-05])\n",
      "tensor([-2.0378e-05])\n",
      "tensor([1.0381e-05])\n",
      "tensor([-2.0099e-05])\n",
      "tensor([1.0172e-05])\n",
      "tensor([-1.9811e-05])\n",
      "tensor([9.9134e-06])\n",
      "tensor([-1.9561e-05])\n",
      "tensor([9.8250e-06])\n",
      "tensor([-1.9239e-05])\n",
      "tensor([9.7390e-06])\n",
      "tensor([-1.8921e-05])\n",
      "tensor([9.5970e-06])\n",
      "tensor([-1.8628e-05])\n",
      "tensor([9.5194e-06])\n",
      "tensor([-1.8312e-05])\n",
      "tensor([9.2852e-06])\n",
      "tensor([-1.8094e-05])\n",
      "tensor([9.0717e-06])\n",
      "tensor([-1.7856e-05])\n",
      "tensor([8.8692e-06])\n",
      "tensor([-1.7605e-05])\n",
      "tensor([8.8958e-06])\n",
      "tensor([-1.7261e-05])\n",
      "tensor([8.7665e-06])\n",
      "tensor([-1.7017e-05])\n",
      "tensor([8.7046e-06])\n",
      "tensor([-1.6718e-05])\n",
      "tensor([8.5874e-06])\n",
      "tensor([-1.6478e-05])\n",
      "tensor([8.4943e-06])\n",
      "tensor([-1.6209e-05])\n",
      "tensor([8.4157e-06])\n",
      "tensor([-1.5924e-05])\n",
      "tensor([8.2179e-06])\n",
      "tensor([-1.5733e-05])\n",
      "tensor([8.0168e-06])\n",
      "tensor([-1.5523e-05])\n",
      "tensor([7.8037e-06])\n",
      "tensor([-1.5343e-05])\n",
      "tensor([7.5677e-06])\n",
      "tensor([-1.5163e-05])\n",
      "tensor([7.6334e-06])\n",
      "tensor([-1.4845e-05])\n",
      "tensor([7.5613e-06])\n",
      "tensor([-1.4610e-05])\n",
      "tensor([7.4733e-06])\n",
      "tensor([-1.4378e-05])\n",
      "tensor([7.3778e-06])\n",
      "tensor([-1.4175e-05])\n",
      "tensor([7.3089e-06])\n",
      "tensor([-1.3923e-05])\n",
      "tensor([7.2326e-06])\n",
      "tensor([-1.3692e-05])\n",
      "tensor([7.0194e-06])\n",
      "tensor([-1.3552e-05])\n",
      "tensor([6.8325e-06])\n",
      "tensor([-1.3394e-05])\n",
      "tensor([6.6640e-06])\n",
      "tensor([-1.3220e-05])\n",
      "tensor([6.4909e-06])\n",
      "tensor([-1.3052e-05])\n",
      "tensor([6.4964e-06])\n",
      "tensor([-1.2812e-05])\n",
      "tensor([6.5639e-06])\n",
      "tensor([-1.2530e-05])\n",
      "tensor([6.3789e-06])\n",
      "tensor([-1.2365e-05])\n",
      "tensor([6.2872e-06])\n",
      "tensor([-1.2195e-05])\n",
      "tensor([6.2519e-06])\n",
      "tensor([-1.1981e-05])\n",
      "tensor([6.1895e-06])\n",
      "tensor([-1.1794e-05])\n",
      "tensor([6.1168e-06])\n",
      "tensor([-1.1602e-05])\n",
      "tensor([6.0374e-06])\n",
      "tensor([-1.1412e-05])\n",
      "tensor([5.9709e-06])\n",
      "tensor([-1.1222e-05])\n",
      "tensor([5.8024e-06])\n",
      "tensor([-1.1109e-05])\n",
      "tensor([5.6330e-06])\n",
      "tensor([-1.0981e-05])\n",
      "tensor([5.4591e-06])\n",
      "tensor([-1.0858e-05])\n",
      "tensor([5.2679e-06])\n",
      "tensor([-1.0748e-05])\n",
      "tensor([5.3230e-06])\n",
      "tensor([-1.0534e-05])\n",
      "tensor([5.3825e-06])\n",
      "tensor([-1.0292e-05])\n",
      "tensor([5.1946e-06])\n",
      "tensor([-1.0184e-05])\n",
      "tensor([5.2743e-06])\n",
      "tensor([-9.9508e-06])\n",
      "tensor([5.2585e-06])\n",
      "tensor([-9.7615e-06])\n",
      "tensor([5.1573e-06])\n",
      "tensor([-9.6367e-06])\n",
      "tensor([5.1380e-06])\n",
      "tensor([-9.4601e-06])\n",
      "tensor([5.0936e-06])\n",
      "tensor([-9.3075e-06])\n",
      "tensor([4.9816e-06])\n",
      "tensor([-9.1954e-06])\n",
      "tensor([4.9298e-06])\n",
      "tensor([-9.0375e-06])\n",
      "tensor([4.8937e-06])\n",
      "tensor([-8.8774e-06])\n",
      "tensor([4.7355e-06])\n",
      "tensor([-8.7903e-06])\n",
      "tensor([4.5751e-06])\n",
      "tensor([-8.7056e-06])\n",
      "tensor([4.4171e-06])\n",
      "tensor([-8.6267e-06])\n",
      "tensor([4.2355e-06])\n",
      "tensor([-8.5554e-06])\n",
      "tensor([4.0668e-06])\n",
      "tensor([-8.4818e-06])\n",
      "tensor([4.1467e-06])\n",
      "tensor([-8.2836e-06])\n",
      "tensor([4.2254e-06])\n",
      "tensor([-8.0930e-06])\n",
      "tensor([4.0760e-06])\n",
      "tensor([-8.0023e-06])\n",
      "tensor([4.1328e-06])\n",
      "tensor([-7.8172e-06])\n",
      "tensor([4.2046e-06])\n",
      "tensor([-7.6315e-06])\n",
      "tensor([3.9215e-06])\n",
      "tensor([-7.6343e-06])\n",
      "tensor([3.8948e-06])\n",
      "tensor([-7.5120e-06])\n",
      "tensor([3.8606e-06])\n",
      "tensor([-7.3902e-06])\n",
      "tensor([3.7850e-06])\n",
      "tensor([-7.3009e-06])\n",
      "tensor([3.7834e-06])\n",
      "tensor([-7.1593e-06])\n",
      "tensor([3.7203e-06])\n",
      "tensor([-7.0526e-06])\n",
      "tensor([3.6638e-06])\n",
      "tensor([-6.9528e-06])\n",
      "tensor([3.6689e-06])\n",
      "tensor([-6.8133e-06])\n",
      "tensor([3.6275e-06])\n",
      "tensor([-6.6917e-06])\n",
      "tensor([3.5784e-06])\n",
      "tensor([-6.5880e-06])\n",
      "tensor([3.5308e-06])\n",
      "tensor([-6.4719e-06])\n",
      "tensor([3.3776e-06])\n",
      "tensor([-6.4364e-06])\n",
      "tensor([3.2437e-06])\n",
      "tensor([-6.3800e-06])\n",
      "tensor([3.0675e-06])\n",
      "tensor([-6.3659e-06])\n",
      "tensor([2.9166e-06])\n",
      "tensor([-6.3259e-06])\n",
      "tensor([3.0048e-06])\n",
      "tensor([-6.1637e-06])\n",
      "tensor([2.8514e-06])\n",
      "tensor([-6.1282e-06])\n",
      "tensor([2.9250e-06])\n",
      "tensor([-5.9746e-06])\n",
      "tensor([3.0333e-06])\n",
      "tensor([-5.8159e-06])\n",
      "tensor([2.9064e-06])\n",
      "tensor([-5.7573e-06])\n",
      "tensor([2.9392e-06])\n",
      "tensor([-5.6362e-06])\n",
      "tensor([3.0205e-06])\n",
      "tensor([-5.4921e-06])\n",
      "tensor([2.9093e-06])\n",
      "tensor([-5.4233e-06])\n",
      "tensor([2.9686e-06])\n",
      "tensor([-5.2866e-06])\n",
      "tensor([2.9444e-06])\n",
      "tensor([-5.2088e-06])\n",
      "tensor([2.9089e-06])\n",
      "tensor([-5.1401e-06])\n",
      "tensor([2.9280e-06])\n",
      "tensor([-5.0343e-06])\n",
      "tensor([2.8831e-06])\n",
      "tensor([-4.9754e-06])\n",
      "tensor([2.8463e-06])\n",
      "tensor([-4.8953e-06])\n",
      "tensor([2.8370e-06])\n",
      "tensor([-4.8150e-06])\n",
      "tensor([2.7919e-06])\n",
      "tensor([-4.7446e-06])\n",
      "tensor([2.7611e-06])\n",
      "tensor([-4.6727e-06])\n",
      "tensor([2.7717e-06])\n",
      "tensor([-4.5722e-06])\n",
      "tensor([2.7131e-06])\n",
      "tensor([-4.5154e-06])\n",
      "tensor([2.6944e-06])\n",
      "tensor([-4.4465e-06])\n",
      "tensor([2.6653e-06])\n",
      "tensor([-4.3539e-06])\n",
      "tensor([2.6430e-06])\n",
      "tensor([-4.2807e-06])\n",
      "tensor([2.6186e-06])\n",
      "tensor([-4.1943e-06])\n",
      "tensor([2.5853e-06])\n",
      "tensor([-4.1222e-06])\n",
      "tensor([2.4510e-06])\n",
      "tensor([-4.1212e-06])\n",
      "tensor([2.3207e-06])\n",
      "tensor([-4.1233e-06])\n",
      "tensor([2.1803e-06])\n",
      "tensor([-4.1167e-06])\n",
      "tensor([2.0407e-06])\n",
      "tensor([-4.1139e-06])\n",
      "tensor([1.9112e-06])\n",
      "tensor([-4.1103e-06])\n",
      "tensor([1.7481e-06])\n",
      "tensor([-4.1249e-06])\n",
      "tensor([1.8650e-06])\n",
      "tensor([-3.9954e-06])\n",
      "tensor([1.7448e-06])\n",
      "tensor([-3.9776e-06])\n",
      "tensor([1.7895e-06])\n",
      "tensor([-3.9126e-06])\n",
      "tensor([1.6539e-06])\n",
      "tensor([-3.9030e-06])\n",
      "tensor([1.7954e-06])\n",
      "tensor([-3.7527e-06])\n",
      "tensor([1.6313e-06])\n",
      "tensor([-3.7805e-06])\n",
      "tensor([1.7231e-06])\n",
      "tensor([-3.6585e-06])\n",
      "tensor([1.8488e-06])\n",
      "tensor([-3.5299e-06])\n",
      "tensor([1.7000e-06])\n",
      "tensor([-3.5376e-06])\n",
      "tensor([1.8062e-06])\n",
      "tensor([-3.4200e-06])\n",
      "tensor([1.6400e-06])\n",
      "tensor([-3.4362e-06])\n",
      "tensor([1.7443e-06])\n",
      "tensor([-3.3185e-06])\n",
      "tensor([1.8582e-06])\n",
      "tensor([-3.1966e-06])\n",
      "tensor([1.7043e-06])\n",
      "tensor([-3.2061e-06])\n",
      "tensor([1.8179e-06])\n",
      "tensor([-3.0808e-06])\n",
      "tensor([1.6859e-06])\n",
      "tensor([-3.0717e-06])\n",
      "tensor([1.7888e-06])\n",
      "tensor([-2.9605e-06])\n",
      "tensor([1.5330e-06])\n",
      "tensor([-3.0368e-06])\n",
      "tensor([1.6415e-06])\n",
      "tensor([-2.9110e-06])\n",
      "tensor([1.6052e-06])\n",
      "tensor([-2.8973e-06])\n",
      "tensor([1.5740e-06])\n",
      "tensor([-2.8721e-06])\n",
      "tensor([1.6280e-06])\n",
      "tensor([-2.7748e-06])\n",
      "tensor([1.6018e-06])\n",
      "tensor([-2.7442e-06])\n",
      "tensor([1.5683e-06])\n",
      "tensor([-2.7200e-06])\n",
      "tensor([1.5324e-06])\n",
      "tensor([-2.7094e-06])\n",
      "tensor([1.5143e-06])\n",
      "tensor([-2.6750e-06])\n",
      "tensor([1.5338e-06])\n",
      "tensor([-2.6159e-06])\n",
      "tensor([1.5271e-06])\n",
      "tensor([-2.5660e-06])\n",
      "tensor([1.5131e-06])\n",
      "tensor([-2.5269e-06])\n",
      "tensor([1.4782e-06])\n",
      "tensor([-2.5077e-06])\n",
      "tensor([1.5043e-06])\n",
      "tensor([-2.4402e-06])\n",
      "tensor([1.4775e-06])\n",
      "tensor([-2.4112e-06])\n",
      "tensor([1.4729e-06])\n",
      "tensor([-2.3637e-06])\n",
      "tensor([1.4377e-06])\n",
      "tensor([-2.3390e-06])\n",
      "tensor([1.4158e-06])\n",
      "tensor([-2.3076e-06])\n",
      "tensor([1.4075e-06])\n",
      "tensor([-2.2663e-06])\n",
      "tensor([1.3832e-06])\n",
      "tensor([-2.2354e-06])\n",
      "tensor([1.3728e-06])\n",
      "tensor([-2.1949e-06])\n",
      "tensor([1.3587e-06])\n",
      "tensor([-2.1585e-06])\n",
      "tensor([1.3668e-06])\n",
      "tensor([-2.1037e-06])\n",
      "tensor([1.3468e-06])\n",
      "tensor([-2.0684e-06])\n",
      "tensor([1.3252e-06])\n",
      "tensor([-2.0346e-06])\n",
      "tensor([1.2870e-06])\n",
      "tensor([-2.0225e-06])\n",
      "tensor([1.3121e-06])\n",
      "tensor([-1.9546e-06])\n",
      "tensor([1.2775e-06])\n",
      "tensor([-1.9293e-06])\n",
      "tensor([1.2558e-06])\n",
      "tensor([-1.8950e-06])\n",
      "tensor([1.2472e-06])\n",
      "tensor([-1.8566e-06])\n",
      "tensor([1.2209e-06])\n",
      "tensor([-1.8266e-06])\n",
      "tensor([1.2135e-06])\n",
      "tensor([-1.7829e-06])\n",
      "tensor([1.1082e-06])\n",
      "tensor([-1.8117e-06])\n",
      "tensor([1.1006e-06])\n",
      "tensor([-1.7641e-06])\n",
      "tensor([9.7632e-07])\n",
      "tensor([-1.7986e-06])\n",
      "tensor([9.7539e-07])\n",
      "tensor([-1.7582e-06])\n",
      "tensor([8.0356e-07])\n",
      "tensor([-1.8217e-06])\n",
      "tensor([8.1823e-07])\n",
      "tensor([-1.7640e-06])\n",
      "tensor([6.8819e-07])\n",
      "tensor([-1.8052e-06])\n",
      "tensor([6.7143e-07])\n",
      "tensor([-1.7707e-06])\n",
      "tensor([5.7038e-07])\n",
      "tensor([-1.7968e-06])\n",
      "tensor([7.9436e-07])\n",
      "tensor([-1.6379e-06])\n",
      "tensor([6.8540e-07])\n",
      "tensor([-1.6615e-06])\n",
      "tensor([5.5338e-07])\n",
      "tensor([-1.6991e-06])\n",
      "tensor([6.2975e-07])\n",
      "tensor([-1.6509e-06])\n",
      "tensor([5.2428e-07])\n",
      "tensor([-1.6757e-06])\n",
      "tensor([6.0367e-07])\n",
      "tensor([-1.6287e-06])\n",
      "tensor([5.0268e-07])\n",
      "tensor([-1.6558e-06])\n",
      "tensor([5.9878e-07])\n",
      "tensor([-1.5878e-06])\n",
      "tensor([4.6077e-07])\n",
      "tensor([-1.6291e-06])\n",
      "tensor([6.1322e-07])\n",
      "tensor([-1.5286e-06])\n",
      "tensor([4.5472e-07])\n",
      "tensor([-1.5798e-06])\n",
      "tensor([5.9442e-07])\n",
      "tensor([-1.4876e-06])\n",
      "tensor([7.1566e-07])\n",
      "tensor([-1.4041e-06])\n",
      "tensor([5.7905e-07])\n",
      "tensor([-1.4518e-06])\n",
      "tensor([7.1316e-07])\n",
      "tensor([-1.3691e-06])\n",
      "tensor([5.7695e-07])\n",
      "tensor([-1.4135e-06])\n",
      "tensor([6.8895e-07])\n",
      "tensor([-1.3313e-06])\n",
      "tensor([5.7020e-07])\n",
      "tensor([-1.3629e-06])\n",
      "tensor([6.6496e-07])\n",
      "tensor([-1.3030e-06])\n",
      "tensor([5.4319e-07])\n",
      "tensor([-1.3332e-06])\n",
      "tensor([6.3772e-07])\n",
      "tensor([-1.2715e-06])\n",
      "tensor([5.2160e-07])\n",
      "tensor([-1.2996e-06])\n",
      "tensor([6.2003e-07])\n",
      "tensor([-1.2364e-06])\n",
      "tensor([5.0204e-07])\n",
      "tensor([-1.2649e-06])\n",
      "tensor([6.1165e-07])\n",
      "tensor([-1.1917e-06])\n",
      "tensor([4.8836e-07])\n",
      "tensor([-1.2334e-06])\n",
      "tensor([6.0169e-07])\n",
      "tensor([-1.1534e-06])\n",
      "tensor([4.7975e-07])\n",
      "tensor([-1.1903e-06])\n",
      "tensor([6.0309e-07])\n",
      "tensor([-1.1031e-06])\n",
      "tensor([4.7206e-07])\n",
      "tensor([-1.1449e-06])\n",
      "tensor([5.8918e-07])\n",
      "tensor([-1.0631e-06])\n",
      "tensor([6.9622e-07])\n",
      "tensor([-1.0042e-06])\n",
      "tensor([5.5681e-07])\n",
      "tensor([-1.0437e-06])\n",
      "tensor([6.6601e-07])\n",
      "tensor([-9.7105e-07])\n",
      "tensor([5.3446e-07])\n",
      "tensor([-1.0067e-06])\n",
      "tensor([6.4389e-07])\n",
      "tensor([-9.3217e-07])\n",
      "tensor([5.2887e-07])\n",
      "tensor([-9.6860e-07])\n",
      "tensor([6.5553e-07])\n",
      "tensor([-8.7361e-07])\n",
      "tensor([5.4983e-07])\n",
      "tensor([-9.0632e-07])\n",
      "tensor([6.3831e-07])\n",
      "tensor([-8.4148e-07])\n",
      "tensor([5.1101e-07])\n",
      "tensor([-8.8318e-07])\n",
      "tensor([6.2899e-07])\n",
      "tensor([-8.0283e-07])\n",
      "tensor([5.2544e-07])\n",
      "tensor([-8.2533e-07])\n",
      "tensor([6.2108e-07])\n",
      "tensor([-7.6744e-07])\n",
      "tensor([4.7981e-07])\n",
      "tensor([-8.0618e-07])\n",
      "tensor([5.8505e-07])\n",
      "tensor([-7.2928e-07])\n",
      "tensor([7.1654e-07])\n",
      "tensor([-6.4241e-07])\n",
      "tensor([5.8761e-07])\n",
      "tensor([-6.8365e-07])\n",
      "tensor([6.7259e-07])\n",
      "tensor([-6.2917e-07])\n",
      "tensor([5.8272e-07])\n",
      "tensor([-6.5076e-07])\n",
      "tensor([6.8586e-07])\n",
      "tensor([-5.7608e-07])\n",
      "tensor([4.5373e-07])\n",
      "tensor([-6.8860e-07])\n",
      "tensor([5.7620e-07])\n",
      "tensor([-6.0361e-07])\n",
      "tensor([6.7911e-07])\n",
      "tensor([-5.3330e-07])\n",
      "tensor([4.4022e-07])\n",
      "tensor([-6.4343e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([5.4640e-07])\n",
      "tensor([-5.7230e-07])\n",
      "tensor([1.0235], requires_grad=True) tensor([1.9690], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "tasa_aprendizaje = 1e-1\n",
    "n_epocas = 1000\n",
    "\n",
    "torch.manual_seed(42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=dispositivo)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=dispositivo)\n",
    "\n",
    "for epoca in range(n_epocas):\n",
    "    y_sombrero = a + b * x_entrenamiento_tensor\n",
    "    error = y_entrenamiento_tensor - y_sombrero\n",
    "    perdida = (error ** 2).mean()\n",
    "\n",
    "    # No más computación manual de los gradientes! \n",
    "    # a_grad = -2 * error.mean()\n",
    "    # b_grad = -2 * (x_tensor * error).mean()\n",
    "    \n",
    "    # ¡Solo le decimos a PyTorch que trabaje hacia ATRÁS desde la pérdida especificada!\n",
    "    perdida.backward()\n",
    "    # Vamos a ver los gradientes calculados...\n",
    "    print(a.grad)\n",
    "    print(b.grad)\n",
    "    \n",
    "    # ¿Qué pasa con la ACTUALIZACIÓN de los parámetros? No tan rapido\n",
    "    \n",
    "    # PRIMER INTENTO\n",
    "    # AttributeError: 'NoneType' object has no attribute 'zero_'\n",
    "    # a = a - lr * a.grad\n",
    "    # b = b - lr * b.grad\n",
    "    # print(a)\n",
    "\n",
    "    # SEGUNDO INTENTO\n",
    "    # RuntimeError: a leaf Variable that requires grad has been used in an in-place operation.\n",
    "    # a -= lr * a.grad\n",
    "    # b -= lr * b.grad        \n",
    "    \n",
    "    # TERCER INTENTO\n",
    "    # Necesitamos usar NO_GRAD para mantener la actualización fuera del cálculo del gradiente\n",
    "    # ¿Porqué es eso? Se reduce al GRAFO DINÁMICO que utiliza PyTorch...\n",
    "    with torch.no_grad():\n",
    "        a -= tasa_aprendizaje * a.grad\n",
    "        b -= tasa_aprendizaje * b.grad\n",
    "    \n",
    "    # PyTorch está \"aferrado\" a sus gradientes calculados, necesitamos decirle que lo deje ir...\n",
    "    a.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "    \n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el primer intento, obtendremos el extraño error a continuación ... pero podemos obtener una pista de lo que está sucediendo mirando el tensor en sí, una vez más \"perdimos\" el gradiente al reasignar los resultados de la actualización a nuestros parámetros. Por lo tanto, el atributo `grad` resulta ser `None` y genera el error ...\n",
    "\n",
    ">\\# PRIMER INTENTO  \n",
    ">tensor([0.7518], device='cuda:0', grad_fn=\\<SubBackward0\\>)  \n",
    ">AttributeError: 'NoneType' object has no attribute 'zero_'\n",
    "\n",
    "Luego lo cambiamos ligeramente, usando una asignación en sitio de Python en nuestro segundo intento. Y, una vez más, PyTorch se queja al respecto y genera un error.\n",
    "\n",
    ">\\# SEGUNDO INTENTO  \n",
    ">RuntimeError: a leaf Variable that requires grad has been used in an in-place operation.\n",
    "\n",
    "*¡¿Por qué?! Resulta ser un caso de \"demasiado de algo bueno\". El culpable es la capacidad de PyTorch de construir un grafo de cálculo dinámico a partir de cada operación de Python que involucra cualquier tensor que calcule gradiente o sus dependencias.*\n",
    "\n",
    "*Profundizaremos en el funcionamiento interno del grafo de cálculo dinámico en la siguiente sección.*\n",
    "\n",
    "Entonces, ¿cómo le decimos a PyTorch que \"retroceda\" y nos permita actualizar nuestros parámetros sin perder el tiempo con su elegante gráfico de cálculo dinámico? Para eso sirve torch.no_grad (). Nos permite realizar operaciones regulares de Python en tensores, independientemente del grafo de cálculo de PyTorch.\n",
    "\n",
    "Finalmente, logramos ejecutar con éxito nuestro modelo y obtener los parámetros resultantes.\n",
    "\n",
    ">\\# TERCER INTENTO  \n",
    ">tensor(\\[1.0235\\], device='cuda:0', requires_grad=True)  \n",
    ">tensor(\\[1.9690\\], device='cuda:0', requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grafo de cálculo dinámico\n",
    "\n",
    ">*Desafortunadamente, a nadie se le puede decir cuál es el grafo de cálculo dinámico. Tienes que verlo por ti mismo ”. Morfeo*\n",
    "\n",
    "El paquete `PyTorchViz` y su método `make_dot(variable)` nos permite visualizar fácilmente un grafo asociado con una variable Python dada.\n",
    "\n",
    "Entonces, sigamos con el mínimo: dos tensores (calculando el gradiente) para nuestros parámetros, predicciones, errores y pérdidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"172pt\" height=\"171pt\"\n",
       " viewBox=\"0.00 0.00 171.50 171.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 167)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-167 167.5,-167 167.5,4 -4,4\"/>\n",
       "<!-- 139688181722384 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>139688181722384</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"118,-21 26,-21 26,0 118,0 118,-21\"/>\n",
       "<text text-anchor=\"middle\" x=\"72\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 139688182209808 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>139688182209808</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"54,-92 0,-92 0,-57 54,-57 54,-92\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1)</text>\n",
       "</g>\n",
       "<!-- 139688182209808&#45;&gt;139688181722384 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>139688182209808&#45;&gt;139688181722384</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M39.535,-56.6724C45.4798,-48.2176 52.5878,-38.1085 58.6352,-29.5078\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"61.5714,-31.4169 64.4601,-21.2234 55.8452,-27.3906 61.5714,-31.4169\"/>\n",
       "</g>\n",
       "<!-- 139688182209680 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>139688182209680</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"163.5,-85 72.5,-85 72.5,-64 163.5,-64 163.5,-85\"/>\n",
       "<text text-anchor=\"middle\" x=\"118\" y=\"-71.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 139688182209680&#45;&gt;139688181722384 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>139688182209680&#45;&gt;139688181722384</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M110.404,-63.9317C103.7191,-54.6309 93.821,-40.8597 85.7479,-29.6276\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"88.4395,-27.3753 79.761,-21.2979 82.7553,-31.4608 88.4395,-27.3753\"/>\n",
       "</g>\n",
       "<!-- 139688182607760 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>139688182607760</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"145,-163 91,-163 91,-128 145,-128 145,-163\"/>\n",
       "<text text-anchor=\"middle\" x=\"118\" y=\"-135.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1)</text>\n",
       "</g>\n",
       "<!-- 139688182607760&#45;&gt;139688182209680 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>139688182607760&#45;&gt;139688182209680</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M118,-127.9494C118,-118.058 118,-105.6435 118,-95.2693\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"121.5001,-95.0288 118,-85.0288 114.5001,-95.0289 121.5001,-95.0288\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f0bb0789f10>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=dispositivo)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=dispositivo)\n",
    "\n",
    "y_sombrero = a + b * x_entrenamiento_tensor\n",
    "error = y_entrenamiento_tensor - y_sombrero\n",
    "perdida = (error ** 2).mean()\n",
    "make_dot(y_sombrero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si llamamos a `make_dot(y_sombrero)` obtendremos el grafo más a la izquierda en la Figura a continuación:\n",
    "\n",
    "![](../figuras/grafo_dinamico_calculo.png)\n",
    "\n",
    "Echemos un vistazo más de cerca a sus componentes:\n",
    "\n",
    "- **cuadros azules**: corresponden a los tensores que utilizamos como parámetros, los que le pedimos a PyTorch que calcule los gradientes;\n",
    "- **cuadro gris**: una operación de Python que involucra un tensor de calculo de gradiente o sus dependencias;\n",
    "- **cuadro verde**: igual que el cuadro gris, excepto que es el punto de partida para el cálculo de gradientes (suponiendo que el método `backward()` se llama desde la variable utilizada para visualizar el grafo): se calculan de abajo hacia arriba en un grafico.\n",
    "\n",
    "Si desplegamos grafo para las variables de `error` (centro) y `perdida` (derecha), la única diferencia entre ellas y la primera es el número de pasos intermedios (recuadros grises).\n",
    "\n",
    "Ahora, observe más de cerca el cuadro verde del grafo de la izquierda: hay dos flechas apuntando a él, ya que está sumando dos variables, $a$ y $b * x$. Parece obvio, ¿verdad?\n",
    "\n",
    "Luego, mira el cuadro gris del mismo grafo: está realizando una multiplicación, a saber, $b * x$. ¡Pero solo hay una flecha apuntando a ella! La flecha proviene del cuadro azul que corresponde a nuestro parámetro $b$.\n",
    "\n",
    "¿Por qué no tenemos un cuadro para nuestros datos $x$? La respuesta es: ¡no calculamos gradientes para ello! Entonces, aunque hay más tensores involucrados en las operaciones realizadas por el grafo de cálculo, solo muestra los tensores de computación en gradiente y sus dependencias.\n",
    "\n",
    "¿Qué sucedería con el gráfico de cálculo si establece `require_grad` en `False` para nuestro parámetro $a$?\n",
    "\n",
    "![](../figuras/grafo_dinamico_calculo_2.png)\n",
    "\n",
    "Como era de esperar, el cuadro azul correspondiente al parámetro $a$ ya no existe. Simplemente: no gradientes, no grafo.\n",
    "\n",
    "Lo mejor del grafo de computación dinámica es el hecho de que puede hacerlo tan complejo como lo desee. Incluso puede usar declaraciones de flujo de control (por ejemplo, declaraciones if) para controlar el flujo de los gradientes (¡obviamente!) :-)\n",
    "\n",
    "La Figura a continuación muestra un ejemplo de esto. Y sí, sé que el cálculo en sí mismo es completamente una tontería ...\n",
    "\n",
    "![](../figuras/grafo_dinamico_calculo_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizador\n",
    "\n",
    "Hasta ahora, hemos estado actualizando manualmente los parámetros utilizando los gradientes calculados. Probablemente esté bien para dos parámetros ... ¡¿pero qué pasaría si tuviéramos muchos?! Utilizamos uno de los optimizadores de PyTorch, como SGD o Adam.\n",
    "\n",
    "Un optimizador toma los parámetros que queremos actualizar, la tasa de aprendizaje que queremos usar (¡y posiblemente muchos otros hiperparámetros también!) Y realiza las actualizaciones a través de su método `step()`.\n",
    "\n",
    "Además, ya no necesitamos poner a cero los gradientes uno por uno. ¡Solo invocamos el método `zero_grad()` del optimizador y eso es todo!\n",
    "\n",
    "En el siguiente código, creamos un optimizador de Descenso de gradiente estocástico (SGD) para actualizar nuestros parámetros $a$ y $b$.\n",
    "\n",
    "*No se deje engañar por el nombre del optimizador: si utilizamos todos los datos de entrenamiento a la vez para la actualización, como lo estamos haciendo en el código, el optimizador está realizando un descenso de gradiente por lotes, a pesar de su nombre.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3367], requires_grad=True) tensor([0.1288], requires_grad=True)\n",
      "tensor([1.0235], requires_grad=True) tensor([1.9690], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=dispositivo)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=dispositivo)\n",
    "print(a, b)\n",
    "\n",
    "tasa_aprendizaje = 1e-1\n",
    "n_epocas = 1000\n",
    "\n",
    "# Defines a SGD optimizer to update the parameters\n",
    "optimizador = optim.SGD([a, b], lr=tasa_aprendizaje)\n",
    "\n",
    "for epoca in range(n_epocas):\n",
    "    y_sombrero = a + b * x_entrenamiento_tensor\n",
    "    error = y_entrenamiento_tensor - y_sombrero\n",
    "    perdida = (error ** 2).mean()\n",
    "\n",
    "    perdida.backward()    \n",
    "    \n",
    "    # No mas actualizaciones manuales!\n",
    "    # with torch.no_grad():\n",
    "    #     a -= lr * a.grad\n",
    "    #     b -= lr * b.grad\n",
    "    optimizador.step()\n",
    "    \n",
    "    # No más decirle a PyTorch que deje ir los gradientes!\n",
    "    # a.grad.zero_()\n",
    "    # b.grad.zero_()\n",
    "    optimizador.zero_grad()\n",
    "    \n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifiquemos nuestros dos parámetros, antes y después, solo para asegurarnos de que todo sigue funcionando bien:\n",
    "\n",
    ">\\#ANTES: a, b   \n",
    ">tensor(\\[0.3367\\], requires_grad=True)  \n",
    ">tensor(\\[0.1288\\], requires_grad=True)  \n",
    ">\\#DESPUES: a, b  \n",
    ">tensor(\\[1.0235\\], requires_grad=True)  \n",
    ">tensor(\\[1.9690\\], requires_grad=True)\n",
    "\n",
    "¡Genial! Hemos optimizado el proceso de optimización :-) ¿Qué queda?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pérdida\n",
    "\n",
    "Ahora abordamos el cálculo de pérdidas. Como se esperaba, PyTorch nos cubrió una vez más. Hay muchas funciones de pérdida para elegir, dependiendo de la tarea en cuestión. Como el nuestro es una regresión, estamos utilizando la pérdida del error cuadrático medio (MSE).\n",
    "\n",
    "*Tenga en cuenta que* `nn.MSELoss` *en realidad crea una función de pérdida para nosotros, NO es la función de pérdida en sí. Además, puede especificar un método de reducción para aplicar, es decir, cómo desea agregar los resultados para puntos individuales: puede promediarlos `(reduction='mean')` o simplemente sumarlos `(reduction='sum')`*.\n",
    "\n",
    "Luego usamos la función de pérdida creada más adelante, para calcular la pérdida dadas nuestras predicciones y nuestras etiquetas.\n",
    "\n",
    "Nuestro código se ve así ahora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3367], requires_grad=True) tensor([0.1288], requires_grad=True)\n",
      "tensor([1.0235], requires_grad=True) tensor([1.9690], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=dispositivo)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=dispositivo)\n",
    "print(a, b)\n",
    "\n",
    "tasa_aprendizaje = 1e-1\n",
    "n_epocas = 1000\n",
    "\n",
    "# Define una funcion de pérdida MSE\n",
    "perdida_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "optimizador = optim.SGD([a, b], lr=tasa_aprendizaje)\n",
    "\n",
    "for epoca in range(n_epocas):\n",
    "    y_sombrero = a + b * x_entrenamiento_tensor\n",
    "    \n",
    "    # No mas calculo manual de la pérdida!\n",
    "    # error = y_tensor - y_sombrero\n",
    "    # perdida = (error ** 2).mean()\n",
    "    perdida = perdida_fn(y_entrenamiento_tensor, y_sombrero)\n",
    "\n",
    "    perdida.backward()    \n",
    "    optimizador.step()\n",
    "    optimizador.zero_grad()\n",
    "    \n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este punto, solo queda una pieza de código para cambiar: las predicciones. Es hora de presentar la forma en que PyTorch implementa una ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo\n",
    "\n",
    "En PyTorch, un modelo está representado por una clase normal de Python que hereda de la clase `Module`.\n",
    "\n",
    "Los métodos más fundamentales que necesita implementar son:\n",
    "\n",
    "- `__init __(self)`: define las partes que componen el modelo, en nuestro caso, dos parámetros, $a$ y $b$.\n",
    "\n",
    ">Sin embargo, no está limitado a definir parámetros ... los modelos también pueden contener otros modelos (o capas) como sus atributos, por lo que puede anidarlos fácilmente. Veremos un ejemplo de esto en breve también.\n",
    "\n",
    "- `forward(self, x)`: realiza el cálculo real, es decir, genera una predicción, dada la entrada x.\n",
    "\n",
    ">Sin embargo, NO debe llamar al método `forward(x)`. Debe llamar a todo el modelo en sí, como en el `modelo(x)` para realizar un pase hacia adelante y predecir la salida.\n",
    "\n",
    "Construyamos un modelo adecuado (pero simple) para nuestra tarea de regresión. Debe tener un aspecto como este:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegresionLinealManual(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Para hacer que \"a\" y \"b\" sean parámetros reales del modelo, necesitamos envolverlos con nn.Parameter\n",
    "        self.a = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        self.b = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Calcula las salidas / predicciones\n",
    "        return self.a + self.b * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el método `__init__`, definimos nuestros dos parámetros, $a$ y $b$, usando la clase `Parameter()`, para decirle a PyTorch que estos tensores deben considerarse parámetros del modelo del que son un atributo.\n",
    "\n",
    "¿Por qué debería importarnos eso? Al hacerlo, podemos usar el método `parameter()` de nuestro modelo para recuperar un iterador sobre todos los parámetros del modelo, incluso aquellos parámetros de modelos anidados, que podemos usar para alimentar nuestro optimizador (¡en lugar de crear una lista de parámetros nosotros mismos!).\n",
    "\n",
    "Además, podemos obtener los valores actuales para todos los parámetros utilizando el método `state_dict()` de nuestro modelo.\n",
    "\n",
    ">IMPORTANTE: necesitamos enviar nuestro modelo al mismo dispositivo donde están los datos. Si nuestros datos están hechos de tensores de GPU, nuestro modelo también debe \"vivir\" dentro de la GPU.\n",
    "\n",
    "Podemos usar todos estos útiles métodos  para cambiar nuestro código, que debería verse así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('a', tensor([0.3367])), ('b', tensor([0.1288]))])\n",
      "OrderedDict([('a', tensor([1.0235])), ('b', tensor([1.9690]))])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Ahora podemos crear un modelo y enviarlo de inmediato al dispositivo\n",
    "modelo = RegresionLinealManual().to(dispositivo)\n",
    "# También podemos inspeccionar sus parámetros usando su state_dict\n",
    "print(modelo.state_dict())\n",
    "\n",
    "tasa_aprendizaje = 1e-1\n",
    "n_epocas = 1000\n",
    "\n",
    "perdida_fn = nn.MSELoss(reduction='mean')\n",
    "optimizador = optim.SGD(modelo.parameters(), lr=tasa_aprendizaje)\n",
    "\n",
    "for epoca in range(n_epocas):\n",
    "    # ¡¿Que es esto?!\n",
    "    modelo.train()\n",
    "\n",
    "    # No mas predicciones manuales!\n",
    "    # y_sombrero = a + b * x_tensor\n",
    "    y_sombrero = modelo(x_entrenamiento_tensor)\n",
    "    \n",
    "    perdida = perdida_fn(y_entrenamiento_tensor, y_sombrero)\n",
    "    perdida.backward()    \n",
    "    optimizador.step()\n",
    "    optimizador.zero_grad()\n",
    "    \n",
    "print(modelo.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, las declaraciones impresas se verán así: los valores finales para los parámetros ayb siguen siendo los mismos, por lo que todo está bien :-)\n",
    "\n",
    ">OrderedDict(\\[('a', tensor(\\[0.3367\\], device='cuda:0')), ('b', tensor(\\[0.1288\\], device='cuda:0'))])  \n",
    ">OrderedDict(\\[('a', tensor(\\[1.0235\\], device='cuda:0')), ('b', tensor(\\[1.9690\\], device='cuda:0'))])\n",
    "\n",
    "Espero que hayas notado una declaración en particular en el código, a la que le asigné un comentario \"¿Qué es esto?!\" - `modelo.train()`.\n",
    "\n",
    ">En PyTorch, los modelos tienen un método `train()` que, algo decepcionante, NO realiza un paso de entrenamiento. Su único propósito es establecer el modelo en modo de entrenamiento. ¿Porque es esto importante? Algunos modelos pueden utilizar mecanismos como el *Dropout*, por ejemplo, que tienen comportamientos distintos en las fases de entrenamiento y evaluación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red Neuronal para reconocer los digitos de MNIST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir una transformación para normalizaz la data\n",
    "transformacion = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "\n",
    "# Bajar y cargar la data de entrenamiento \n",
    "datos_entrenamiento = datasets.MNIST('../../MNIST_data/', download=True, train=True, transform=transformacion)\n",
    "datos_validacion = datasets.MNIST('../../MNIST_data/', download=True, train=False, transform=transformacion)\n",
    "cargador_entrenamiento = torch.utils.data.DataLoader(datos_entrenamiento, batch_size=64, shuffle=True)\n",
    "cargador_validación = torch.utils.data.DataLoader(datos_validacion, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explorar la data\n",
    "\n",
    "Veamos la forma de las imágenes y las etiquetas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(cargador_entrenamiento)\n",
    "imagenes, etiquetas = dataiter.next()\n",
    "print(type(imagenes))\n",
    "print(imagenes.shape)\n",
    "print(etiquetas.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La forma de las imágenes es, `torch.Size([64,1,28,28])`, lo que sugiere que hay 64 imágenes en cada lote y cada imagen tiene una dimensión de 28 x 28 píxeles. Del mismo modo, las etiquetas tienen forma de `torch.Size([64])`. Las 64 imágenes deben tener 64 etiquetas respectivamente.\n",
    "\n",
    "Vamos a mostrar una imagen del conjunto de entrenamiento, por ejemplo, la primera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(imagenes[0].numpy().squeeze(), cmap='gray_r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto generará una cuadrícula de imágenes en un orden aleatorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 60 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "figura = plt.figure()\n",
    "num_de_imagenes = 60\n",
    "for index in range(1, num_de_imagenes + 1):\n",
    "    plt.subplot(6, 10, index)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(imagenes[index].numpy().squeeze(), cmap='gray_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definir La Red Neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construiremos la siguiente red, ya que puede ver que contiene una capa de entrada (la primera capa), una capa de salida de diez neuronas (o unidades, los círculos) y dos capas ocultas en el medio.\n",
    "\n",
    "![](../figuras/mlp_mnist.png)\n",
    "\n",
    "El módulo `torch.nn` de PyTorch nos permite construir la red anterior de manera muy simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (5): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Detalles de las capas de la red neuronal\n",
    "tamaño_entrada = 784\n",
    "tamaño_ocultas = [128, 64]\n",
    "tamaño_salida = 10\n",
    "\n",
    "# Construir la red neuronal\n",
    "modelo = nn.Sequential(nn.Linear(tamaño_entrada, tamaño_ocultas[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(tamaño_ocultas[0], tamaño_ocultas[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(tamaño_ocultas[1], tamaño_salida),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "print(modelo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El `nn.Secuential` envuelve las capas en la red. Hay tres capas lineales con activación ReLU (una función simple que permite el paso de valores positivos, mientras que los valores negativos se modifican a cero). La capa de salida es una capa lineal con activación `LogSoftmax` porque este es un problema de clasificación.\n",
    "\n",
    "Técnicamente, una función `LogSoftmax` es el logaritmo de una función `Softmax` como su nombre lo dice y se ve así, como se muestra a continuación.\n",
    "\n",
    "$$LogSoftMax(x_i)=log\\left(\\frac{exp(x_i)}{\\sum_j exp(x_i)}\\right)$$\n",
    "\n",
    "A continuación, definimos la pérdida de probabilidad logarítmica negativa. La cual es útil para entrenar un problema de clasificación con $C$ clases. Juntas, `LogSoftmax()` y `NLLLoss()` actúan como la pérdida de entropía cruzada como se muestra en el diagrama de arquitectura de red anterior.\n",
    "\n",
    "Además, debe preguntarse por qué tenemos 784 unidades en la primera capa. ¡Bueno! Es porque aplanamos cada imagen antes de enviarla dentro de la red neuronal. (28 x 28 = 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterio = nn.NLLLoss()\n",
    "imagenes, etiquetas = next(iter(cargador_entrenamiento))\n",
    "imagenes = imagenes.view(imagenes.shape[0], -1)\n",
    "\n",
    "logps = modelo(imagenes)\n",
    "perdida = criterio(logps, etiquetas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajuste de pesos\n",
    "\n",
    "Una red neuronal aprende iterando varias veces sobre los datos disponibles. El término aprender se refieren al ajuste de los pesos de la red para minimizar la pérdida. Vamos a visualizar cómo funciona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes del paso hacia atrás : \n",
      " None\n",
      "Después del paso hacia atrás: \n",
      " tensor([[-0.0006, -0.0006, -0.0006,  ..., -0.0006, -0.0006, -0.0006],\n",
      "        [ 0.0001,  0.0001,  0.0001,  ...,  0.0001,  0.0001,  0.0001],\n",
      "        [ 0.0027,  0.0027,  0.0027,  ...,  0.0027,  0.0027,  0.0027],\n",
      "        ...,\n",
      "        [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
      "        [-0.0005, -0.0005, -0.0005,  ..., -0.0005, -0.0005, -0.0005],\n",
      "        [-0.0001, -0.0001, -0.0001,  ..., -0.0001, -0.0001, -0.0001]])\n"
     ]
    }
   ],
   "source": [
    "print('Antes del paso hacia atrás : \\n', modelo[0].weight.grad)\n",
    "\n",
    "perdida.backward()\n",
    "\n",
    "print('Después del paso hacia atrás: \\n', modelo[0].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "# Optimizadores requieren requieren los parámetros a optimizar\n",
    "# y la tasa de aprendizaje\n",
    "optimizador = optim.SGD(modelo.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos iniciales -  Parameter containing:\n",
      "tensor([[ 0.0105, -0.0315, -0.0323,  ...,  0.0224,  0.0098,  0.0058],\n",
      "        [ 0.0026,  0.0204,  0.0326,  ..., -0.0088,  0.0078,  0.0345],\n",
      "        [ 0.0132, -0.0150,  0.0087,  ...,  0.0101, -0.0190,  0.0026],\n",
      "        ...,\n",
      "        [-0.0124, -0.0155,  0.0187,  ...,  0.0155, -0.0157,  0.0257],\n",
      "        [-0.0182, -0.0258,  0.0084,  ..., -0.0257,  0.0018,  0.0028],\n",
      "        [ 0.0191, -0.0252, -0.0264,  ...,  0.0268, -0.0224, -0.0237]],\n",
      "       requires_grad=True)\n",
      "Gradiente - tensor([[-0.0001, -0.0001, -0.0001,  ..., -0.0001, -0.0001, -0.0001],\n",
      "        [ 0.0008,  0.0008,  0.0008,  ...,  0.0008,  0.0008,  0.0008],\n",
      "        [ 0.0007,  0.0007,  0.0007,  ...,  0.0007,  0.0007,  0.0007],\n",
      "        ...,\n",
      "        [ 0.0018,  0.0018,  0.0018,  ...,  0.0018,  0.0018,  0.0018],\n",
      "        [-0.0030, -0.0030, -0.0030,  ..., -0.0030, -0.0030, -0.0030],\n",
      "        [-0.0001, -0.0001, -0.0001,  ..., -0.0001, -0.0001, -0.0001]])\n"
     ]
    }
   ],
   "source": [
    "print('Pesos iniciales - ', modelo[0].weight)\n",
    "\n",
    "imagenes, etiquetas = next(iter(cargador_entrenamiento))\n",
    "imagenes.resize_(64, 784)\n",
    "\n",
    "# Limpiar los gradientes, hacer esto porque los gradientes se acumulan\n",
    "optimizador.zero_grad()\n",
    "\n",
    "# Paso hacia adelate, después paso hacia atrás, y luego actualizar los pesos\n",
    "salida = modelo(imagenes)\n",
    "perdida = criterio(salida, etiquetas)\n",
    "perdida.backward()\n",
    "print('Gradiente -', modelo[0].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos actualizados -  Parameter containing:\n",
      "tensor([[ 0.0105, -0.0315, -0.0323,  ...,  0.0224,  0.0098,  0.0058],\n",
      "        [ 0.0026,  0.0204,  0.0326,  ..., -0.0088,  0.0078,  0.0345],\n",
      "        [ 0.0132, -0.0150,  0.0087,  ...,  0.0101, -0.0190,  0.0026],\n",
      "        ...,\n",
      "        [-0.0124, -0.0155,  0.0187,  ...,  0.0155, -0.0157,  0.0257],\n",
      "        [-0.0182, -0.0258,  0.0084,  ..., -0.0257,  0.0018,  0.0028],\n",
      "        [ 0.0191, -0.0252, -0.0264,  ...,  0.0268, -0.0224, -0.0237]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Tomar un paso de optimización y observar los nuevos pesos\n",
    "optimizador.step()\n",
    "print('Pesos actualizados - ', modelo[0].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento de la Red Neuronal\n",
    "\n",
    "Aquí es donde sucede la magia real. Su red neuronal itera sobre el conjunto de entrenamiento y actualiza los pesos. Hacemos uso de `torch.optim`, que es un módulo proporcionado por PyTorch para optimizar el modelo, realizar el descenso de gradiente y actualizar los pesos mediante la propagación hacia atrás. Por lo tanto, en cada época (número de veces que iteramos sobre el conjunto de entrenamiento), veremos una disminución gradual en la pérdida de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca 0 - Perdida entrenamiento: 0.6351858228842205\n",
      "Epoca 1 - Perdida entrenamiento: 0.2793898798112295\n",
      "Epoca 2 - Perdida entrenamiento: 0.2144003163205027\n",
      "Epoca 3 - Perdida entrenamiento: 0.17215760164598284\n",
      "Epoca 4 - Perdida entrenamiento: 0.1434965854677469\n",
      "Epoca 5 - Perdida entrenamiento: 0.1228178322190113\n",
      "Epoca 6 - Perdida entrenamiento: 0.10816096924821228\n",
      "Epoca 7 - Perdida entrenamiento: 0.09568168242066813\n",
      "Epoca 8 - Perdida entrenamiento: 0.08573132533449004\n",
      "Epoca 9 - Perdida entrenamiento: 0.07864445165893845\n",
      "Epoca 10 - Perdida entrenamiento: 0.07174939949408984\n",
      "Epoca 11 - Perdida entrenamiento: 0.06763036986753376\n",
      "Epoca 12 - Perdida entrenamiento: 0.06088813435909813\n",
      "Epoca 13 - Perdida entrenamiento: 0.05638287930604396\n",
      "Epoca 14 - Perdida entrenamiento: 0.05333175350214912\n",
      "\n",
      "Tiempo Entrenamiento (en minutos) = 3.306777811050415\n"
     ]
    }
   ],
   "source": [
    "optimizador = optim.SGD(modelo.parameters(), lr=0.003, momentum=0.9)\n",
    "tiempo_0 = time()\n",
    "epocas = 15\n",
    "for e in range(epocas):\n",
    "    perdida_actual = 0\n",
    "    for imagenes, etiquetas in cargador_entrenamiento:\n",
    "        # Aplanar las imagenes MNIST a un vector de longitud 784\n",
    "        imagenes = imagenes.view(imagenes.shape[0], -1)\n",
    "    \n",
    "        # Paso de entrenamiento\n",
    "        optimizador.zero_grad()\n",
    "        \n",
    "        salida = modelo(imagenes)\n",
    "        perdida = criterio(salida, etiquetas)\n",
    "        \n",
    "        # Aquí es donde el modelo apende mediante la propagación hacia atrás\n",
    "        perdida.backward()\n",
    "        \n",
    "        # Y aquí optimiza sus pesos\n",
    "        optimizador.step()\n",
    "        \n",
    "        perdida_actual += perdida.item()\n",
    "    else:\n",
    "        print(\"Epoca {} - Perdida entrenamiento: {}\".format(e, perdida_actual/len(cargador_entrenamiento)))\n",
    "print(\"\\nTiempo Entrenamiento (en minutos) =\",(time()-tiempo_0)/60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación del Modelo\n",
    "\n",
    "Ya casi hemos terminado con nuestro trabajo. El modelo está listo, pero primero tenemos que evaluarlo. Crear una función de utilidad ver_clasificacion() para mostrar la imagen y las probabilidades de clase que se predijeron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ver_clasificacion(imagen, ps):\n",
    "    ''' Función para ver una imagen y sus clases predichas.\n",
    "    '''\n",
    "    ps = ps.data.numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "    ax1.imshow(imagen.resize_(1, 28, 28).numpy().squeeze())\n",
    "    ax1.axis('off')\n",
    "    ax2.barh(np.arange(10), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(10))\n",
    "    ax2.set_yticklabels(np.arange(10))\n",
    "    ax2.set_title('Probabilidad Clase')\n",
    "    ax2.set_xlim(0, 1.1)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dígito Predicho = 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "imagenes, etiquetas = next(iter(cargador_validación))\n",
    "\n",
    "imagen = imagenes[0].view(1, 784)\n",
    "# Apagar gradientes para acelerar esta parte\n",
    "with torch.no_grad():\n",
    "    logps = modelo(imagen)\n",
    "\n",
    "# Salida de la red son probabilidades logarítmicas, \n",
    "# se necesita tomar el exponente para obtener probabilidades\n",
    "ps = torch.exp(logps)\n",
    "probab = list(ps.numpy()[0])\n",
    "print(\"Dígito Predicho =\", probab.index(max(probab)))\n",
    "ver_clasificacion(imagen.view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probar el modelo con los datos de validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de Imágenes Probadas = 10000\n",
      "\n",
      "Exactitud del Modelo = 0.9721\n"
     ]
    }
   ],
   "source": [
    "conteo_correcto, conteo_total = 0, 0\n",
    "for imagenes,etiquetas in cargador_validación:\n",
    "  for i in range(len(etiquetas)):\n",
    "    imagen = imagenes[i].view(1, 784)\n",
    "    # Apagar gradientes para acelerar esta parte\n",
    "    with torch.no_grad():\n",
    "        logps = modelo(imagen)\n",
    "\n",
    "    # Salida de la red son probabilidades logarítmicas, \n",
    "    # se necesita tomar el exponente para obtener probabilidades\n",
    "    ps = torch.exp(logps)\n",
    "    probab = list(ps.numpy()[0])\n",
    "    etiqueta_predicha = probab.index(max(probab))\n",
    "    etiqueta_verdadera = etiquetas.numpy()[i]\n",
    "    if(etiqueta_verdadera == etiqueta_predicha):\n",
    "      conteo_correcto += 1\n",
    "    conteo_total += 1\n",
    "\n",
    "print(\"Número de Imágenes Probadas =\", conteo_total)\n",
    "print(\"\\nExactitud del Modelo =\", (conteo_correcto/conteo_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
